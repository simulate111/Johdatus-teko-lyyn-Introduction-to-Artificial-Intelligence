{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Conv2D, MaxPooling2D, Input\n",
    "plt.rcParams['figure.figsize'] = (13, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style> \n",
    "table td, table th, table tr {text-align:left !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>521160P Introduction to artificial intelligence<br><br>Exercise #5<br><br>Reinforcement and deep learning<br></center>\n",
    "\n",
    "This exercise explores game strategies for Blackjack card game through reinforcement learning and teaches convolutional neural networks to identify categories of images through deep learning. **Look return deadlines from moodle** (Return instructions for this exercise are sligthly different from previous ones). It is possible to get 4 points from this exercise (2.0p + 2.0p).\n",
    "\n",
    "If you have any questions related to exercises or you face any problems during this exercise please use moodle forum for **programming exercise 5**.\n",
    "\n",
    "(*if you are using a 32-bit system and are having problems installing tensorflow, one option is to uninstall the current Python 3.8, install [Miniconda3 (64-bit)](https://docs.conda.io/en/latest/miniconda. html) (during installation, select All Users to install and add Miniconda3 to the PATH environment variable), run the command conda install jupyter matplotlib numpy scikit-image scikit-learn and the command conda install -c hesi_m tensorflow from the command line as an administrator. This version of tensorflow is unofficial and does not support the latest changes to tensorflow. However, it manages to use the basic functions of tensorflow. Perform this intermediate step only if you do not have tensorflow installed and your system is 32-bit.*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First fill in your group information (name and student number)**\n",
    "\n",
    "# Group member information :\n",
    "\n",
    "* **Member 1 :** `First_name Surname 1234567 `\n",
    "* **Member 2 :** `Maija Meikäläinen 2345678 `\n",
    "* **Member 3 :** `... `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1. Blackjack card game with reinforcement learning\n",
    "\n",
    "In this task, game strategies for the Blackjack card game are solved through reinforcement learning. Let's first go through the rules of the simplified version of the game.\n",
    "\n",
    "### Blackjack card game rules\n",
    "\n",
    "At the beginning of the game, the player is dealt two cards face up and the dealer one card face up and one card face down. In a blackjack card game, the number cards 2-10 are worth their number, the picture cards are worth ten, and the ace is one or eleven, depending on desire. The player plays against the dealer and his goal is to get the sum of his cards higher than the sum of the dealer's cards. The sum of the cards must be as close as possible to 21, but must not go over, in which case the player loses the game. If a player's sum of cards on two starting cards is 21 as in Figure 1, he has blackjack. With this, the player wins all the hands of the dealer except the dealer's own blackjack, in which case the game ends in a draw. The player raises more cards from the deck until he is satisfied with the amount of his cards.\n",
    "\n",
    "<br>\n",
    "<div style=\"width:image width px; font-size:80%; text-align:center;\">\n",
    "    <center>\n",
    "    <img src='imgs\\blackjack.jpg' width='600' height='auto' style='padding-bottom:0.5em;' />\n",
    "    </center>\n",
    "    <span>Figure 1. In a blackjack card game, the player has blackjack in the starting hand. </span>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "Next, the turn moves to the dealer. The dealer must draw cards until the sum of his cards is 17 or greater. For example, if the sum of the dealer's cards is 18 and the sum of the player's cards is 19, the player wins the game. The player also wins the game when the sum of the dealer's cards exceeds 21. At the end of the game, the possible results are 17-17, 18-18, 19-19, 20-20 or 21-21.\n",
    "\n",
    "Next, auxiliary functions are created for the implementation of the rules of the game and for the reinforcement learning algorithms.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_hand():\n",
    "    \"\"\"\n",
    "    This function initializes the hand. The sum of the cards in the empty hand is 0 and does not include an ace. \n",
    "    \"\"\"\n",
    "    empty_hand = (0, False)\n",
    "    return empty_hand\n",
    "\n",
    "def is_unsused_aces(hand):\n",
    "    \"\"\"\n",
    "    This function checks to see if the hand contains unused aces.\n",
    "    An ace is still available if the sum of the cards in the hand is less than 21 and a value of 11 has been used for the ace. \n",
    "    \"\"\"\n",
    "    sum_of_cards, is_aces = hand    \n",
    "    return (is_aces and (sum_of_cards + 10) <= 21)\n",
    "\n",
    "def calc_sum_of_cards(hand):\n",
    "    \"\"\"\n",
    "    This function calculates the sum of the cards in the hand. If an ace is available, 10 is added to the sum of the cards. \n",
    "    \"\"\"\n",
    "    sum_of_cards, is_aces = hand\n",
    "    if is_unsused_aces(hand):\n",
    "        return sum_of_cards + 10\n",
    "    return sum_of_cards\n",
    "\n",
    "def update_hand(hand, card_value):\n",
    "    \"\"\"\n",
    "    This function updates the hand when new card is added\n",
    "    \"\"\"\n",
    "    sum_of_cards, is_aces = hand\n",
    "    sum_of_cards += card_value\n",
    "    if card_value == 1:\n",
    "        is_aces = True\n",
    "    updated_hand = (sum_of_cards, is_aces)\n",
    "    return updated_hand\n",
    "        \n",
    "def draw_card():\n",
    "    \"\"\"\n",
    "    In this function, a card is drawn from a deck of infinitely many cards.\n",
    "    Thus, after the draw, each card still has the same probability of being selected.\n",
    "    the ace is 1, the number cards 2-10 are worth their number, and the picture cards are 10.\n",
    "    \"\"\"\n",
    "    card_value = random.randint(1,13)\n",
    "    if (card_value > 10):\n",
    "        card_value = 10\n",
    "    return card_value\n",
    "\n",
    "def deal_player_starting_hand():\n",
    "    \"\"\"\n",
    "    This function deals the player's starting hand.\n",
    "    In terms of game strategy, it is interesting to find out how a player plays when the sum of his cards is over 11. \n",
    "    \"\"\"\n",
    "    hand = initialize_hand()\n",
    "    for i in range(2):\n",
    "        card_value = draw_card()\n",
    "        hand = update_hand(hand, card_value)\n",
    "    while calc_sum_of_cards(hand) < 11:\n",
    "        card_value = draw_card()\n",
    "        hand = update_hand(hand, card_value)\n",
    "    return hand\n",
    "\n",
    "def deal_dealer_starting_hand():\n",
    "    \"\"\"\n",
    "    This function deals dealer's starting hand \n",
    "    \"\"\"    \n",
    "    hand = initialize_hand()\n",
    "    card_value = draw_card()\n",
    "    hand = update_hand(hand, card_value)\n",
    "    return hand, card_value\n",
    "\n",
    "def pelaa_dealer_vuoro(hand):\n",
    "    \"\"\"\n",
    "    In this function, the dealer raises a new card until the sum of the cards in his/her hand is 17 or greater. \n",
    "    \"\"\"    \n",
    "    while calc_sum_of_cards(hand) < 17:\n",
    "        card_value = draw_card()\n",
    "        hand = update_hand(hand, card_value)\n",
    "    return hand\n",
    "\n",
    "def generate_states():\n",
    "    \"\"\"\n",
    "    This function returns all possible states of the game in the list. The value of the dealer's card is between 1-10,\n",
    "    the sum of the player's cards is 11-21 and the player's hand either contains an ace or not. \n",
    "    \"\"\"\n",
    "    states = []\n",
    "    for dealer_card_value in range(1,11):\n",
    "        for player_sum_of_cards in range(11,22):\n",
    "            states.append((dealer_card_value, player_sum_of_cards, False))\n",
    "            states.append((dealer_card_value, player_sum_of_cards, True))\n",
    "    return states\n",
    "\n",
    "def start_game_from_state(state):\n",
    "    \"\"\"\n",
    "    This function starts the game when it is given state information at the beginning of the game. \n",
    "    \"\"\"\n",
    "    dealer_card_value, player_sum_of_cards, is_aces = state\n",
    "    if is_aces:\n",
    "        player_sum_of_cards -= 10\n",
    "    player_hand = (player_sum_of_cards, is_aces)\n",
    "    dealer_hand = initialize_hand()\n",
    "    dealer_hand = update_hand(dealer_hand, dealer_card_value)\n",
    "    return dealer_card_value, dealer_hand, player_hand\n",
    "\n",
    "def initialize_state_motion_table_values():\n",
    "    \"\"\"\n",
    "    This function initializes the values of the state motion table with zeros. Movement True means that the player raises a new card and\n",
    "    movement False, the player decides his turn, being happy with his cards. \n",
    "    \"\"\"\n",
    "    states = generate_states()\n",
    "    state_motion_table = {}\n",
    "    for state in states:\n",
    "        state_motion_table[(state,True)] = 0.0\n",
    "        state_motion_table[(state,False)] = 0.0\n",
    "    return state_motion_table\n",
    "\n",
    "def change_state_from_hand(card_value, player_hand):\n",
    "    \"\"\"\n",
    "    This function converts player's hand into state\n",
    "    \"\"\"\n",
    "    player_sum_of_cards = calc_sum_of_cards(player_hand)\n",
    "    is_aces = is_unsused_aces(player_hand)\n",
    "    return (card_value, player_sum_of_cards, is_aces)\n",
    "\n",
    "def find_random_state(states):\n",
    "    \"\"\"\n",
    "    In this function, the state is randomly selected from all possible states.\n",
    "    \"\"\"\n",
    "    random_index = random.randint(0,len(states)-1)\n",
    "    state = states[random_index]\n",
    "    return state\n",
    "\n",
    "def choose_random_move():\n",
    "    \"\"\"\n",
    "    In this function, one of the moves is randomly selected to raise a new card and end the turn. \n",
    "    \"\"\"\n",
    "    random_num = random.randint(1,2)\n",
    "    return random_num == 1\n",
    "\n",
    "def choose_best_move(Q, state):\n",
    "    \"\"\"\n",
    "    In this function, the better of the two possible movements is selected based on the Q values.\n",
    "    \"\"\"\n",
    "    if Q[(state,True)] > Q[(state,False)]:\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "def give_game_feedback (player_hand, dealer_hand):\n",
    "    \"\"\"\n",
    "    This function provides feedback on the outcome of the game. \n",
    "    \"\"\"\n",
    "    player_sum_of_cards = calc_sum_of_cards(player_hand)\n",
    "    dealer_sum_of_cards = calc_sum_of_cards(dealer_hand)\n",
    "    if player_sum_of_cards > 21:\n",
    "        return -1\n",
    "    if dealer_sum_of_cards > 21:\n",
    "        return 1\n",
    "    if player_sum_of_cards < dealer_sum_of_cards:\n",
    "        return -1\n",
    "    if player_sum_of_cards == dealer_sum_of_cards:\n",
    "        return 0\n",
    "    if player_sum_of_cards > dealer_sum_of_cards:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ace has a big impact on game strategy, so it’s important to separate the states when a player has an ace in their hand and when they don’t.\n",
    "\n",
    "The state of the game contains three values: the value of the dealer's starting hand, the sum of the cards in the player's hand, and whether the player's hand contains an ace. In Game State, a player has the ability to make two types of moves: 1) raise one more card or 2) decide their turn while being happy with the amount of their cards. The choice of movement can be made randomly or use previously learned information to benefit from the selection.\n",
    "\n",
    "Feedback on the outcome of the game is given according to how it ended. when the player wins the feedback is 1, at the end of the game the tie is 0 and when the dealer wins the feedback is -1.\n",
    "\n",
    "### Monte Carlo-simulaatio\n",
    "\n",
    "Next, a Monte Carlo simulation is performed. Since the Blackjack card game is a stationary process, the learning rate is $\\alpha=\\frac{1}{N (S_{t}, A_{t})}$. The Monte Carlo simulation updates the Q values according to Equation 1. \n",
    "\n",
    "<br>\n",
    "\\begin{equation}\n",
    "Q(S_{t}, A_{t}) \\leftarrow Q(S_{t}, A_{t}) + \\alpha \\: (R_{t}-Q(S_{t}, A_{t})) \\tag{1}\n",
    "\\end{equation}\n",
    "<br>\n",
    "\n",
    "The Epsilon-Greedy strategy alternates between random and best movement selection, for example, by selecting nine out of ten times the best-observed movement and one out of ten random movement.\n",
    "\n",
    "Implement the Monte Carlo simulation update rule to calculate Q-values and the epsilon-Greedy strategy to select motion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_monte_carlo_simulation(N, Q, state_motion_pair, R):\n",
    "    \"\"\"\n",
    "    This function updates the number of visited state motion pairs in Table N and Q Table values\n",
    "    \"\"\"\n",
    "    # Add the number of state-owned business pairs visited to the business-to-business pair in Table One \n",
    "    N[state_motion_pair] += 1\n",
    "    #-------YOUR CODE HERE--------\n",
    "    # update state motion pair new Q-value with Monte Carlo simulation update rule when alpha = 1 / N [state_motion_pair] \n",
    "    Q[state_motion_pair] = \n",
    "    #--------------------------------\n",
    "    return N, Q\n",
    "\n",
    "def use_epsilon_greedy_strategy(Q, state, epsilon):\n",
    "    \"\"\"\n",
    "    This function uses the epsilon-Greedy strategy for motion selection.\n",
    "    The percentage indicated by the epsilon value indicates how often random motion is selected.\n",
    "    Otherwise, the best possible motion is selected. \n",
    "    \"\"\"\n",
    "    #-------YOUR CODE HERE--------\n",
    "    # Pick random value between 0 and 1 (Hint: random.random ()) \n",
    "    \n",
    "    # If the drawn number is less than epsilon \n",
    "    \n",
    "        # The function returns a randomly selected move, i.e. the function choose_random_move() \n",
    "        \n",
    "    # Otherwise, the function returns the best motion, ie the function choose_best_move(Q, state) \n",
    "    \n",
    "    #--------------------------------\n",
    "\n",
    "def run_monte_carlo_simulation(number_of_iterations, epsilon):\n",
    "    \"\"\"\n",
    "    This function performs a Monte Carlo simulation for a Blackjack card game.     \n",
    "    \"\"\"  \n",
    "    # Initialize with zeros the values of the state motion pairs and how many times the state motion pairs have been visited \n",
    "    Q = initialize_state_motion_table_values()\n",
    "    N = initialize_state_motion_table_values()\n",
    "    # Generate states\n",
    "    states = generate_states()\n",
    "    for i in range(number_of_iterations):\n",
    "        state_motion_pairs = []\n",
    "        # Select state and move at random \n",
    "        state = find_random_state(states)\n",
    "        move = choose_random_move()\n",
    "        dealer_card_value, dealer_hand, player_hand = start_game_from_state(state)\n",
    "        # Adding a state motion pair to the list \n",
    "        state_motion_pairs.append((state, move))\n",
    "        # The player draws new cards until the sum of the cards exceeds 21 \n",
    "        while move:\n",
    "            player_hand = update_hand(player_hand, draw_card())\n",
    "            if calc_sum_of_cards(player_hand) > 21:\n",
    "                break\n",
    "            state = change_state_from_hand(dealer_card_value, player_hand)\n",
    "            move = use_epsilon_greedy_strategy(Q, state, epsilon)\n",
    "            state_motion_pairs.append((state, move))\n",
    "        # The dealer's turn to play when the player decides to end his/her turn\n",
    "        dealer_hand = pelaa_dealer_vuoro(dealer_hand)\n",
    "        # Calculate the value of the feedback \n",
    "        R = give_game_feedback(player_hand, dealer_hand)\n",
    "        # Updating Q values \n",
    "        for state_motion_pair in state_motion_pairs:\n",
    "            N, Q = update_monte_carlo_simulation(N, Q, state_motion_pair, R)\n",
    "    return Q\n",
    "\n",
    "# Perform a Monte Carlo simulation\n",
    "# It will take a while to run this step \n",
    "Q_monte_carlo = run_monte_carlo_simulation(1000000, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the Q-values determined by the Monte Carlo simulation on a three-dimensional graph of moves, lift a new card and decide the turn when the player has an ace in his/her hand and when there are none. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_q_values_on_graph(Q, X, Y, Z, header, is_aces, pick_card, azim=-60):\n",
    "    \"\"\"\n",
    "    In this function, the Q-values are plotted on a three-dimensional graph for a given game movement and the condition of whether there is an ace in the hand.\n",
    "    \"\"\"\n",
    "    for key, Q_arvo in Q.items():\n",
    "        if key[0][2] == is_aces and key[1] == pick_card:\n",
    "            Z[key[0][1]-11][key[0][0]-1] = Q_arvo\n",
    "    fig = plt.figure(figsize=(12,7))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.view_init(azim=azim)\n",
    "    surface = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, vmin=-1, vmax=1, cmap=cm.bwr, linewidth=0, antialiased=False)\n",
    "    ax.set_ylabel('Sum of player\\'s cards')\n",
    "    ax.set_xlabel('dealer card value')\n",
    "    ax.set_zlabel('Q value')\n",
    "    ax.set_title(header)\n",
    "    fig.colorbar(surface, shrink=0.5, aspect=5)\n",
    "\n",
    "# Initialize the values of the graph axes \n",
    "X = np.arange(1,11)\n",
    "Y = np.arange(11,22)\n",
    "Z = np.zeros((11,10))\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "\n",
    "# Q-values with Monte Carlo simulation to end a turn when a player has no unused aces \n",
    "header = 'Q-values with Monte Carlo simulation to end a turn when a player has no unused aces'\n",
    "draw_q_values_on_graph(Q_monte_carlo, X, Y, Z, header, False, False)\n",
    "\n",
    "# Q-values in Monte Carlo simulation for raising a new card when a player has no unused aces \n",
    "header = 'Q-values in Monte Carlo simulation for raising a new card when a player has no unused aces '\n",
    "draw_q_values_on_graph(Q_monte_carlo, X, Y, Z, header, False, True, azim=355)\n",
    "\n",
    "# Q-values with Monte Carlo simulation to end a turn when a player has unused aces \n",
    "header = 'Q-values with Monte Carlo simulation to end a turn when a player has unused aces '\n",
    "draw_q_values_on_graph(Q_monte_carlo, X, Y, Z, header, True, False)\n",
    "\n",
    "# Q-values with Monte Carlo simulation for raising a new card when a player has unused aces \n",
    "header = 'Q-values with Monte Carlo simulation for raising a new card when a player has unused aces '\n",
    "draw_q_values_on_graph(Q_monte_carlo, X, Y, Z, header, True, True, azim=355)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game strategy determined by the Monte Carlo simulation is printed by selecting the move with the highest Q value from the state-move table for the states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_game_strategy(Q, algorithm_name):\n",
    "    \"\"\"\n",
    "    This function prints the game strategy by selecting a move with a higher Q value from the moves.\n",
    "    \"\"\"\n",
    "    print('Game strategy solved with {} (X raise the card , * end turn).\\nThe vertical axis shows the sum of the player cards and the horizontal axis the value of the dealer card.'.format(algorithm_name))\n",
    "    for is_aces_bool in [True, False]:\n",
    "        if is_aces_bool:\n",
    "            print('\\nThe player has unused aces\\n')\n",
    "        else:\n",
    "            print('\\nThe player has no unused aces\\n')\n",
    "        for player_hand_value in range(21,10,-1):\n",
    "            for dealer_card in range(1,11):\n",
    "                if (Q[((dealer_card,player_hand_value,is_aces_bool),True)] > Q[((dealer_card,player_hand_value,is_aces_bool),False)]):\n",
    "                    print(' X', end='')\n",
    "                else:\n",
    "                    print(' *', end='')\n",
    "            print(' | {}'.format(player_hand_value))\n",
    "        print('---------------------')\n",
    "        print(' A 2 3 4 5 6 7 8 9 10')\n",
    "\n",
    "# The game strategy determined by the Monte Carlo simulation is printed \n",
    "print_game_strategy(Q_monte_carlo, 'Monte Carlo-simulaatio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If the value of the dealer's card is 4, the sum of the player's cards is 15 and the player has no unused aces, what should he do based on the game strategy determined by the Monte Carlo simulation?**\n",
    "\n",
    "`Your answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q learning\n",
    "\n",
    "Next, Q-learning is performed. Now use the learning speed value $\\alpha=\\frac{\\alpha}{n(s_{t}, a_{t})}$. Q learning updates Q values in accordance with equation 2. \n",
    "\n",
    "<br>\n",
    "\\begin{equation}\n",
    "Q(S_{t},A_{t}) \\leftarrow Q(S_{t},A_{t}) + \\alpha \\: [R_{t} + \\gamma \\max (Q(S_{t+1},A_{t})) - Q(S_{t},A_{t})] \\tag{2}\n",
    "\\end{equation}\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_q_learning(number_of_iterations, epsilon, learning_speed):\n",
    "    \"\"\"\n",
    "    This function executes a Q learning algorithm for a Blackjack card game.\n",
    "    \"\"\"  \n",
    "    # Initialize the values of the state-move pairs with zeros and how many times the state-move pairs have been visited \n",
    "    Q = initialize_state_motion_table_values()\n",
    "    N = initialize_state_motion_table_values()\n",
    "    # Generate states\n",
    "    states = generate_states()\n",
    "    for i in range(number_of_iterations ):\n",
    "        # Pick state randomly\n",
    "        state = find_random_state(states)\n",
    "        dealer_card_value, dealer_hand, player_hand = start_game_from_state(state)\n",
    "        while (True):\n",
    "            # Choose next move with epsilon greedy strategy\n",
    "            move = use_epsilon_greedy_strategy(Q, state, epsilon)\n",
    "            state_motion_pair = (state, move)\n",
    "            if move:\n",
    "                # The player draws new cards until the sum of the cards exceeds 21 \n",
    "                card_value = draw_card()\n",
    "                player_hand = update_hand(player_hand, card_value)\n",
    "                if calc_sum_of_cards(player_hand) > 21:\n",
    "                    # Update Q value\n",
    "                    N[state_motion_pair] += 1\n",
    "                    Q[state_motion_pair] += (learning_speed/N[state_motion_pair]) * ((-1) - Q[state_motion_pair])\n",
    "                    break\n",
    "                else:\n",
    "                    # Update Q value\n",
    "                    next_state = change_state_from_hand(dealer_card_value, player_hand)\n",
    "                    Q_value_max = Q[(next_state,False)]\n",
    "                    if (Q[(next_state,True)] > Q_value_max):\n",
    "                        Q_value_max = Q[(next_state,True)]\n",
    "                    N[state_motion_pair] += 1\n",
    "                    Q[state_motion_pair] += (learning_speed/N[state_motion_pair]) * (Q_value_max - Q[state_motion_pair])\n",
    "                    # Update state\n",
    "                    state = next_state\n",
    "            else:\n",
    "                # Dealer's turn to play when player ends turn\n",
    "                dealer_hand = pelaa_dealer_vuoro(dealer_hand)\n",
    "                # Calculate feedback value\n",
    "                R = give_game_feedback (player_hand, dealer_hand)\n",
    "                # Update Q-value\n",
    "                N[state_motion_pair] += 1\n",
    "                Q[state_motion_pair] += (learning_speed/N[state_motion_pair]) * (R - Q[state_motion_pair])\n",
    "                break\n",
    "    return Q\n",
    "\n",
    "# Perform Q-learning\n",
    "# It will take a while to run this step\n",
    "Q_q_oppiminen = run_q_learning(1000000, 0.05, 0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the Q-values determined by Q-learning on a three-dimensional graph of moves, lift a new card and end the turn when the player has an ace in her/his hand and when there are none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the values of the graph axes \n",
    "X = np.arange(1,11)\n",
    "Y = np.arange(11,22)\n",
    "Z = np.zeros((11,10))\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "\n",
    "# Q-values with Q-learning to end a turn when a player has no unused aces \n",
    "header = 'Q-values with Q-learning to end a turn when a player has no unused aces '\n",
    "draw_q_values_on_graph(Q_q_oppiminen, X, Y, Z, header, False, False)\n",
    "\n",
    "# Q values with Q-learning to raise a new card when the player has no unused ace \n",
    "header = 'Q-values with Q-learning to raise a new card when the player has no unused aces'\n",
    "draw_q_values_on_graph(Q_q_oppiminen, X, Y, Z, header, False, True, azim=355)\n",
    "\n",
    "# Q-values with Q-learning to end a turn when a player has unused aces \n",
    "header = 'Q-values with Q-learning to end a turn when a player has unused aces'\n",
    "draw_q_values_on_graph(Q_q_oppiminen, X, Y, Z, header, True, False)\n",
    "\n",
    "# Q-values with Q-learning for raising a new card when a player has unused aces \n",
    "header = 'Q-values with Q-learning for raising a new card when a player has unused aces'\n",
    "draw_q_values_on_graph(Q_q_oppiminen, X, Y, Z, header, True, True, azim=355)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game strategy determined by Q-learning is printed by selecting the move with the highest Q-value from the state-move table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print out the game strategy determined by Q-learning \n",
    "print_game_strategy(Q_q_oppiminen, 'Q-oppimise')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One casino promises to return the player's bet twice the winnings of the Blackjack game. In a draw, the stake is returned as is and at a loss the player loses his stake.\n",
    "\n",
    "Finally, let’s look at how well the game strategies figured out outperform that casino based on the payout percentage when both strategies play 100,000 games. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_payback_percentage (Q, number_of_games):\n",
    "    \"\"\"\n",
    "    This function calculates the payback percentage for the game strategy when playing many games in a row. \n",
    "    \"\"\"\n",
    "    payback = 0\n",
    "    states = generate_states()\n",
    "    for i in range(number_of_games):\n",
    "        state = find_random_state(states)\n",
    "        dealer_card_value, dealer_hand, player_hand = start_game_from_state(state)\n",
    "        while True:\n",
    "            move = choose_best_move(Q, state)\n",
    "            state_motion_pair = (state, move)\n",
    "            if move:\n",
    "                # The player draws new cards until the sum of the cards is over 21 \n",
    "                card = draw_card()\n",
    "                player_hand = update_hand(player_hand, card)\n",
    "                if calc_sum_of_cards(player_hand) > 21:\n",
    "                    R = give_game_feedback (player_hand, dealer_hand)\n",
    "                    payback += R\n",
    "                    break\n",
    "            else:\n",
    "                # Dealer's turn to play, after player ends turn\n",
    "                dealer_hand = pelaa_dealer_vuoro(dealer_hand)\n",
    "                # Calculate feedback value\n",
    "                R = give_game_feedback (player_hand, dealer_hand)\n",
    "                payback += R\n",
    "                break\n",
    "    return (number_of_games + payback)/number_of_games\n",
    "\n",
    "print('Payback percentage for the game strategy determined by the Monte Carlo simulation: {} %\\n'.format(round(100*calc_payback_percentage (Q_monte_carlo, 100000),3)))\n",
    "print('Payback percentage for game strategy determined by Q-learning: {} %\\n'.format(round(100*calc_payback_percentage(Q_q_oppiminen, 100000),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which of the game strategies is better? Is it worth playing Blackjack at that casino with these gaming strategies?**\n",
    "\n",
    "`Your answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Teaching convolution neural networks \n",
    "\n",
    "The second task is to train three convolutional neural networks to classify images. In this task the Cifar-10 dataset is used. The dataset contains 60,000 RGB images of size 32 x 32 pixels. The dataset contains 10 classes of objects such as passenger cars, birds and cats. To reduce the training and validation time, a subset of 5000 samples for both training and validation is used. Figure 2 shows example images of all the classes.\n",
    "\n",
    "<br>\n",
    "<div style=\"width:image width px; font-size:80%; text-align:center;\">\n",
    "    <center>\n",
    "    <img src='imgs\\cifar10.png' width='950' height='auto' style='padding-bottom:0.5em;' />\n",
    "    </center>\n",
    "    <span>Figure 2. Example images of data CIFAR-10 from all classes.</span>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "The CIFAR-10 dataset is loaded and divided into teaching and validation datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 data\n",
    "(data_teaching, classes_teaching), (data_validation, classes_validation) = cifar10.load_data()\n",
    "data_teaching, classes_teaching, data_validation, classes_validation = data_teaching[:5000], classes_teaching[:5000], data_validation[:5000], classes_validation[:5000]\n",
    "data_teaching, data_validation = data_teaching.astype('float32'), data_validation.astype('float32')\n",
    "data_teaching /= 255\n",
    "data_validation /= 255\n",
    "\n",
    "# Find ammount of classes\n",
    "class_ammount = len(list(set(classes_teaching.reshape(-1).tolist())))\n",
    "class_names = ['airplane','car','bird','cat','deer','dog','frog','horse','boat','truck']\n",
    "\n",
    "# Transform classes into categorical form with one-hot encoding\n",
    "classes_teaching_categorical = keras.utils.to_categorical(classes_teaching, class_ammount)\n",
    "classes_validation_categorical = keras.utils.to_categorical(classes_validation, class_ammount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning, an epoch is the process of feeding the entire training dataset into the model once. Usually the training requires several epochs. Often the data is fed to the model in batches instead of as individual samples. The model parameters are updated after each batch. For example, if the teaching data has 1024 samples and a batch size is 8 samples, 128 iterations are performed during one epoch.\n",
    "\n",
    "The learning process of a neural network model can be monitored by seeing how the loss value (or classification rate, etc.) developes over epochs. In Figure 3, the red training data classification rate is plotted with two validation classification rates. The training data classification rate keeps getting better while the green and blue curves show the small and large amounts of overfitting, respectively. Training should be stopped when the validation classification accuracy reaches its maximum and validation loss is minimal.\n",
    "\n",
    "<br>\n",
    "<div style=\"width:image width px; font-size:80%; text-align:center;\">\n",
    "    <center>\n",
    "    <img src='imgs\\luokittelutarkkuus.png' width='350' height='auto' style='padding-bottom:0.5em;' />\n",
    "    </center>\n",
    "    <span>Figure 3. The difference between the classification accuracy of teaching data and validation data is directly proportional to the number of administration. </span>\n",
    "</div>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your job is to create and teach three different convolutional neural networks and evaluate their performance. Model Model 1, Model 2 and Model 3 Structures can be found in Table 1. Model 1 has been used for two convolutional layer, activation function, and maximum sub-sampling layer. In the model 2, there are additionally neuron dropout layers and model 3 normalization for a batch. In Table 1, each layer can be found in the brackets within the description of the features after transformation of dimensions\n",
    "\n",
    "Model 1 is pre-implemented as an example for the implementation of models 2 and 3. Use the following for the functions of the Keras library when adding layers: convolution ([Conv2D()](https://keras.io/layers/convolutional/)), activation function ([Activation()](https://keras.io/layers/core/)), max pooling ([Maxpooling2D()](https://keras.io/layers/pooling/), neuron dropout layer ([Dropout()](https://keras.io/layers/core/)), normalization for batch ([BatchNormalization()](https://keras.io/layers/normalization/)) , input layer([Flatten()](https://keras.io/layers/core/)), the hidden layer and the output layer ([Dense()](https://keras.io/layers/core/)).\n",
    "\n",
    "<br>\n",
    "<div style=\"width:image width px; font-size:80%; text-align:left;\">\n",
    "    <span>Table 1. Structures of convulution networks used in this exercise</span>\n",
    "\n",
    "| Model 1 | Model 2 | Model 3 |\n",
    "|   ---   |   ---   |   ---   |\n",
    "| RGB-image (32x32x3) | RGB-image (32x32x3) | RGB-image (32x32x3) |\n",
    "| <br> | <br> | <br> |\n",
    "| 3x3 convolution, 32 layers with zero padding (32x32x32) | 3x3 convolution, 32 layers with zero padding (32x32x32) | 3x3 convolution, 32 layers with zero padding (32x32x32) |\n",
    "| ReLU-activation function (32x32x32) | ReLU-activation function (32x32x32) | ReLU-activation function (32x32x32) |\n",
    "| 3x3 convolution, 32 layers with zero padding (32x32x32) |3x3 convolution, 32 layers with zero padding (32x32x32) | Normalization according to batch(32x32x32) |\n",
    "| ReLu-activation function (32x32x32) | ReLu-activation function (32x32x32) | 3x3 convolution, 32 layers with zero padding (32x32x32) |\n",
    "| 2x2 max pooling (16x16x32) | 2x2 max pooling (16x16x32) | ReLu-activation function (32x32x32) |\n",
    "| <br> | 30 % neuron dropout layer (16x16x32) | Normalization according to batch size (32x32x32) |\n",
    "| 3x3 convolution, 64 layers with zero padding (16x16x64) | <br> | 2x2 max pooling (16x16x32) |\n",
    "| ReLu-activation function (16x16x64) | 3x3 convolution, 64 layers with zero padding (16x16x64) | 30 % neuron dropout layer (16x16x32) |\n",
    "| 3x3 convolution, 64 layers with zero padding (16x16x64) | ReLu-activation function (16x16x64) | <br> |\n",
    "| ReLu-activation function (16x16x64) | 3x3 convolution, 64 layers with zero padding (16x16x64) | 3x3 convolution, 64 layers with zero padding (16x16x64) |\n",
    "| 2x2 max pooling (8x8x64) | ReLu-activation function (16x16x64) | ReLu-activation function (16x16x64) |\n",
    "| <br> | 2x2 max pooling (8x8x64) | Normalization according to batch size (16x16x64) |\n",
    "| input layer (4096) | 40 % neuron dropout layer (8x8x64) | 3x3 convolution, 64 layers with zero padding (16x16x64) |\n",
    "| hidden layer (128) | <br> | ReLu-activation function (16x16x64) |\n",
    "| ReLu-activation function (128) | input layer (4096) | Normalization according to batch size (16x16x64) |\n",
    "| Dense layer (10) | hidden layer (128) | 2x2 max pooling (8x8x64) |\n",
    "| Softmax activation function (10) | ReLu-activation function (128) | 40 % neuron dropout layer (8x8x64) |\n",
    "| <br> | 50 % neuron dropout layer (128) | <br> |\n",
    "| <br> | Dense layer (10) | input layer (4096) |\n",
    "| <br> | Softmax activation function (10) | hidden layer (128) |\n",
    "| <br> | <br> | ReLu-activation function (128) |\n",
    "| <br> | <br> | Normalization according to batch size (128) |\n",
    "| <br> | <br> | 50 % neuron dropout layer (128) |\n",
    "| <br> | <br> | Dense layer (10) |\n",
    "| <br> | <br> | Softmax activation function (10) |\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "An example of a convolutional neural  network model 1 is created and teaching it with CIFAR-10 data 20 cycles when a batch size is 64 samples. You can reduce the size of the batch (the variable *batch_size*) to 32 or 16 samples, if your computer's memory is limited. Draw graph for model 1 and add teaching procesess', teaching data's and validation data's classification accuracies and losses in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_graph_teaching_process(model, model_name):\n",
    "    \"\"\"\n",
    "    This function is drawn to the graph of the classification accuracy of teaching data and validation data and losses as teaching progresses. \n",
    "    \"\"\"\n",
    "    # Draw classification accuracies and losses of teaching process, teaching data and validation data\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(15,15))\n",
    "    periods = np.arange(len(model.history['accuracy']))+1\n",
    "    ax[0].plot(periods, model.history['accuracy'])\n",
    "    ax[0].plot(periods, model.history['val_accuracy'])\n",
    "    teaching_data_accuracy = model.history['accuracy'][-1]\n",
    "    validation_data_accuracy = model.history['val_accuracy'][-1]\n",
    "    teaching_data_loss = model.history['loss'][-1]\n",
    "    validation_data_loss = model.history['val_loss'][-1]\n",
    "    ax[0].set_title('development of teaching data and validation data for model {}'.format(model_name))\n",
    "    ax[0].set_xlabel('Period')\n",
    "    ax[0].set_ylabel('Classification accuracy')\n",
    "    ax[0].set_xticks(np.arange(len(model.history['accuracy']))+1)\n",
    "    ax[0].legend(['Teaching data\\'s classification accuracy', 'validation data\\'s classification accuracy'], loc='upper left')\n",
    "    \n",
    "    ax[1].plot(periods, model.history['loss'])\n",
    "    ax[1].plot(periods, model.history['val_loss'])\n",
    "    ax[1].set_title('Losses of teaching and validation data for model {}'.format(model_name))\n",
    "    ax[1].set_xlabel('Period')\n",
    "    ax[1].set_ylabel('Loss')\n",
    "    ax[1].set_xticks(np.arange(len(model.history['accuracy']))+1)\n",
    "    ax[1].legend(['Teaching data loss', 'Validation data loss'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    # Prints after teaching teaching and validation data's classification accuracies and losses\n",
    "    print('Classification accuracy for teaching data {} after teaching period {} %'.format(periods.size, round(100*teaching_data_accuracy,3)))\n",
    "    print('Classification accuracy for validation data {} after teaching period {} %'.format(periods.size, round(100*validation_data_accuracy,3)))\n",
    "    print('Teaching data loss {} after teaching period {}'.format(periods.size, round(teaching_data_loss,3)))\n",
    "    print('Validation data loss {} after teaching period {}'.format(periods.size, round(validation_data_loss,3)))\n",
    "\n",
    "def create_model1():\n",
    "    \"\"\"\n",
    "    This function creates convolutional network according to model1\n",
    "    \"\"\"\n",
    "    # Initialize model1 consisting of consequent layers\n",
    "    model1 = Sequential(name='model_1')\n",
    "    model1.add(Input(shape=(32,32,3)))\n",
    "\n",
    "    # Add 3x3 convolution with a zero padding produce 32 layers of features \n",
    "    model1.add(Conv2D(32, (3,3), padding='same', name='Conv_1'))\n",
    "    # Add ReLu-activation function\n",
    "    model1.add(Activation('relu', name='ReLU_1'))\n",
    "    # Add 3x3 convolution with a zero padding produce 32 layers of features \n",
    "    model1.add(Conv2D(32, (3,3), padding='same', name='Conv_2'))\n",
    "    # Add ReLu-activation function\n",
    "    model1.add(Activation('relu', name='ReLU_2'))\n",
    "    # Add 2x2 max pooling layer\n",
    "    model1.add(MaxPooling2D(pool_size=(2,2), name='MaxPooling_1'))\n",
    "\n",
    "    # Add 3x3 convolution with zero-padding which produces 64 layers of features\n",
    "    model1.add(Conv2D(64, (3,3), padding='same', name='Conv_3'))\n",
    "    # Add ReLu-activation function\n",
    "    model1.add(Activation('relu', name='ReLU_3'))\n",
    "    # Add 3x3 convolution with zero-padding which produces 64 layers of features\n",
    "    model1.add(Conv2D(64, (3,3), padding='same', name='Conv_4'))\n",
    "    # Add ReLu-activation function\n",
    "    model1.add(Activation('relu', name='ReLU_4'))\n",
    "    # Add 2x2 max pooling layer\n",
    "    model1.add(MaxPooling2D(pool_size=(2,2), name='MaxPooling_2'))\n",
    "\n",
    "    # Add a flatten layer\n",
    "    model1.add(Flatten(name='Flatten_1'))\n",
    "    # Add hidden layer with 128 neurons\n",
    "    model1.add(Dense(128, name='Dense_1'))\n",
    "    # Add ReLu-activation function\n",
    "    model1.add(Activation('relu', name='ReLU_5'))\n",
    "    # Add Dense layer with 10 neurons\n",
    "    model1.add(Dense(10, name='Dense_2'))\n",
    "    # Add Softmax activation function\n",
    "    model1.add(Activation('softmax', name='Softmax'))\n",
    "\n",
    "    # Define categorical crossentropy and ADAM-optimization as error function\n",
    "    model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model1\n",
    "\n",
    "# Create model 1\n",
    "model1 = create_model1()\n",
    "print(model1.summary())\n",
    "\n",
    "# Teaching Model 1 and draw as a graph of teaching process\n",
    "# Running this step might take a while\n",
    "taught_model1 = model1.fit(data_teaching, classes_teaching_categorical, batch_size=64, epochs=20, validation_data=(data_validation, classes_validation_categorical), shuffle=True)\n",
    "draw_graph_teaching_process(taught_model1, '1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Model 2 taught by CIFAR-10 data 20 cycles when a batch size has 64 samples. You can reduce the size of the batch (the variable *batch_size*) to 32 or 16 samples if your computer's memory is limited. Draw graph for model 2 and add teaching procesess', teaching data's and validation data's classification accuracies and losses on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model2():\n",
    "    \"\"\"\n",
    "    This function creates model2\n",
    "    \"\"\"\n",
    "    # Initialize model1 consisting of consequent layers\n",
    "    model2 = Sequential(name='model_2')\n",
    "    model2.add(Input(shape=(32,32,3)))\n",
    "\n",
    "    #-------YOUR CODE HERE--------\n",
    "    # Add 3x3 convolution with zero-padding which produces 32 layers of features\n",
    "    \n",
    "    # Add ReLu-activation function\n",
    "    \n",
    "    # Add 3x3 convolution with zero-padding which produces 32 layers of features\n",
    "    \n",
    "    # Add ReLu-activation function\n",
    "    \n",
    "    # Add 2x2 max pooling layer\n",
    "    \n",
    "    # Add 30% neuron dropout layer\n",
    "    \n",
    "\n",
    "    # Add 3x3 convolution with zero-padding which produces 64 layers of features\n",
    "    \n",
    "    # Add ReLu-activation function\n",
    "    \n",
    "    # Add 3x3 convolution with zero-padding which produces 64 layers of features\n",
    "    \n",
    "    # Add ReLu-activation function\n",
    "    \n",
    "    # Add 2x2 max pooling layer\n",
    "    \n",
    "    # Add 40% neuron dropout layer\n",
    "    \n",
    "\n",
    "    # Add a flatten layer\n",
    "    \n",
    "    # Add a 128 neuron hidden layer\n",
    "    \n",
    "    # Add ReLu-activation function\n",
    "    \n",
    "    # Add 50% neuron dropout layer\n",
    "    \n",
    "    # Add Dense layer with 10 neurons\n",
    "    \n",
    "    # Add Softmax activation function\n",
    "    \n",
    "    #--------------------------------\n",
    "\n",
    "    # Define categorical crossentropy and ADAM-optimization as an error-function\n",
    "    model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model2\n",
    "\n",
    "# Create model 2\n",
    "model2 = create_model2()\n",
    "print(model2.summary())\n",
    "               \n",
    "# Teaching Model 2 and draw as a graph of teaching process\n",
    "# Running this step might take a while\n",
    "taught_model2 = model2.fit(data_teaching, classes_teaching_categorical, batch_size=64, epochs=20, validation_data=(data_validation, classes_validation_categorical), shuffle=True)\n",
    "draw_graph_teaching_process(taught_model2, '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model 3 and teach it with CIFAR-10 data for 20 cycles when batch size is 64 layers. You can reduce variable *batch_size* to 32 or 16 samples if computer's memory is limited. Plot graph for model 3 and add teaching process teaching data and validation data's classification accuracies and losses on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model3():\n",
    "    \"\"\"\n",
    "    Tässä funktiossa luodaan konvoluutioneuroverkko model 3.\n",
    "    \"\"\"\n",
    "    # Alustetaan peräkkäisistä kerroksista koostuva model\n",
    "    model3 = Sequential(name='model3')\n",
    "    model3.add(Input(shape=(32,32,3)))\n",
    "\n",
    "    #-------YOUR CODE HERE--------\n",
    "    # Add 3x3 convolution with zero-padding that produces 32 layers of features\n",
    "    \n",
    "    # Add the ReLu-activation function\n",
    "    \n",
    "    # Add normalization according to batch size\n",
    "\n",
    "    # Add 3x3 convolution with zero-padding that produces 32 layers of features\n",
    "    \n",
    "    # Add the ReLu-activation function\n",
    "    \n",
    "    # Add normalization according to batch size\n",
    "    \n",
    "    # Add 2x2 max pooling layer\n",
    "    \n",
    "    # Add 30% neuron dropout layer\n",
    "    \n",
    "\n",
    "    # Add 3x3 convolution with a zero-padding that produces 64 layers of features\n",
    "    \n",
    "    # Add the ReLu-activation function\n",
    "    \n",
    "    # Add normalization according to batch size\n",
    "    \n",
    "    # Add 3x3 convolution with a zero padding that produces 64 layers of features\n",
    "    \n",
    "    # Add the ReLu-activation function\n",
    "    \n",
    "    # Add normalization according to batch size\n",
    "    \n",
    "    # Add 2x2 max pooling layer\n",
    "    \n",
    "    # Add 40% neuron dropout layer\n",
    "    \n",
    "\n",
    "    # Add a flatten layer\n",
    "    \n",
    "    # Add a hidden layer with 128 neurons\n",
    "    \n",
    "    # Add the ReLu-activation function\n",
    "    \n",
    "    # Add normalization according to batch size\n",
    "    \n",
    "    # Add 50% of the neuron dropout layer\n",
    "    \n",
    "    # Add the Dense layer with 10 neurons\n",
    "    \n",
    "    # Add softmax activation function\n",
    "    \n",
    "    #--------------------------------\n",
    "\n",
    "    # Define categorical crossentropy and ADAM-optimization as an error-function\n",
    "    model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model3\n",
    "\n",
    "# Create model 3\n",
    "model3 = create_model3()\n",
    "print(model3.summary())\n",
    "               \n",
    "# Teaching Model 3 and draw as a graph of teaching process\n",
    "# Running this step might take a while\n",
    "taught_model3 = model3.fit(data_teaching, classes_teaching_categorical, batch_size=64, epochs=20, validation_data=(data_validation, classes_validation_categorical), shuffle=True)\n",
    "draw_graph_teaching_process(taught_model3, '3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which of the taught convolutional neural network models is best? Whihc one on the other hand was most overfitted**\n",
    "\n",
    "`Your answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the teaching of the models, they are tested on the first sample of ValidationDATA 25. Drawing a picture of a validation sample, a category predicted to it, a predicted class posterior probability and brackets the correct category of sample. The color of the image text is blue when the forecast went right and red when it went wrong. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_posterior_probabilities_for_predictions(model, model_name, data_validation, classes_validation, class_names, row_ammount, column_ammount):\n",
    "    \"\"\"\n",
    "    Predict with model most probable class with posteriorprobabilities for test samples\n",
    "    \"\"\"\n",
    "    predicted_classes = model.predict(data_validation)\n",
    "    img_ammount = row_ammount*column_ammount   \n",
    "    plt.figure(figsize=(3*column_ammount, 4*row_ammount)) \n",
    "    plt.suptitle('Largest posterior probability from validation samples predicted with model {} '.format(model_name))\n",
    "    for i in range(img_ammount):\n",
    "        plt.subplot(row_ammount, column_ammount, i+1)\n",
    "        predictions, right_class, img = predicted_classes[i], *classes_validation[i], data_validation[i]\n",
    "        plt.grid(False)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([]) \n",
    "        plt.imshow(img)\n",
    "        predicted_class = np.argmax(predictions)\n",
    "        if predicted_class == right_class:\n",
    "            color = 'blue'\n",
    "        else:\n",
    "            color = 'red'\n",
    "        plt.title('{} {}% \\n({})'.format(class_names[predicted_class], round(100*np.max(predictions),3), class_names[right_class]), color=color)\n",
    "\n",
    "# Draw validation samples in image and predict with model 1 class which got highest posterior probability \n",
    "print_posterior_probabilities_for_predictions(model1, '1', data_validation, classes_validation, class_names, 5, 5)\n",
    "\n",
    "# Draw validation samples in image and predict with model 2 class which got highest posterior probability \n",
    "print_posterior_probabilities_for_predictions(model2, '2', data_validation, classes_validation, class_names, 5, 5)\n",
    "\n",
    "# Draw validation samples in image and predict with model 3 class which got highest posterior probability \n",
    "print_posterior_probabilities_for_predictions(model3, '3', data_validation, classes_validation, class_names, 5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedback\n",
    "\n",
    "Finaly answer following questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How much time did you spend doing this exercise?**\n",
    "\n",
    "`Your answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Did you encounter any problems or challenges while doing the exercise? Were the notebook sufficiently comprehensive instructions for doing the exercise?**\n",
    "\n",
    "`Your answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other feedback related to to this exercise?**\n",
    "\n",
    "`Your answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Returning\n",
    "\n",
    "1. Before you return this notebook **check that values are correct overall**. No need to run again because it takes quite long time. \n",
    "2. Do not clear the printouts and variables, but press the save button after completing Step 1! \n",
    "3. Rename this notebook in following format **`JT_H5_[student_number(s)].ipynb`** (e.g `JT_H5_1234567.ipynb` or if you have group `JT_H5_1234567_2345678_3456789.ipynb`)\n",
    "4. Return **only** solved notebook(`file ending with .ipynb`) to moodle programming exercise 3. Everyone must return file to moodle even if you work in a group. **Don't include working directory or other files** when you return the exercise.\n",
    "5. Finally go answer questions in moodle related to programming exercise 4. Everyone in your group must answer to questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
