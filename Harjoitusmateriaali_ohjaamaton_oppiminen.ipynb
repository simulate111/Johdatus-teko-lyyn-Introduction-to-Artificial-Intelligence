{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from onedrivedownloader import download\n",
    "\n",
    "link2data = 'https://unioulu-my.sharepoint.com/:u:/g/personal/jukmaatt_univ_yo_oulu_fi/EXPart1N1OxAuRlGl_FPkIABfgI6o-7ErGM5vAtwADiG5w?e=1ibEuK'\n",
    "\n",
    "if not os.path.exists('./data'):\n",
    "    print('Downloading data')\n",
    "    download(link2data, filename='./tiedostot.zip', unzip=True, unzip_path='./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>521160P Johdatus Tekoälyyn<br><br>Harjoitusmateriaali<br><br>Ohjaamaton oppiminen<br></center>\n",
    "\n",
    "\n",
    "Kun koneoppimisen algoritmi käyttää ilman lähtömuuttujia sisältävää dataa, on kyseessä ohjaamaton oppiminen. Koska datanäytteille ei ole etukäteen määritelty luokkia, täytyy datan ryhmittely tehdä täysin perustuen sen rakenteisuuteen. Ohjaamaton oppiminen jaetaan klusterointiin ja dimensionaalisuuden vähentämiseen.\n",
    "\n",
    "## Klusterointi\n",
    "\n",
    "Klusterointi on ohjaamattoman oppimisen osa-alue, jota käytetään datan analysoimiseen ja ryhmittelyyn. Klusteroinnin päämääränä on jakaa data pienempiin ryhmiin eli klustereihin, joiden näytteet ovat mahdollisimman samankaltaisia keskenään, mutta eri klustereiden näytteet ovat mahdollisimman erilaisia toisiinsa verrattuna. Datan näytteiden samankaltaisuutta voidaan mitata esimerkiksi euklidisen etäisyyden tai todennäköisyystiheyksien perusteella. Jokaisella eri klusterointiongelmalla ovat omat tunnuspiirteensä, minkä takia ei ole olemassa yleisesti käytettyä menetelmää, joka toimii hyvin kaikissa tilanteissa. Eri klusterointimenetelmät voidaankin jakaa karkeasti sen mukaan, mitä ominaisuuksia ne hyödyntävät.\n",
    "\n",
    "**Hierarkkiset klusterointimenetelmät** ovat joko jakavia tai kokoavia [1]. Jakavissa menetelmissä lähdetään liikkeelle joukosta, johon kaikki näytteet kuuluvat. Näytejoukko jaetaan samankaltaisuuksien perusteella aina pienempiin osajoukkoihin askel kerrallaan, kunnes kaikki näytteet on saatu klusteroitua. Kokoavissa menetelmissä puolestaan lähdetään liikkeelle yksittäisistä näytteistä, jotka pyritään yhdistämään vaiheittain aina suurempiin samankaltaisiin kokonaisuuksiin. Lopuksi voidaan piirtää yksinkertainen puukaavio eli dendrogrammi, joka kuvaa hierarkkisesti näytteiden välisiä samankaltaisuuksia. Kuvassa 1 on muodostettu eräälle klusterointiongelmalle Venn-diagrammit kokoavalla ja jakavalla hierarkkisella klusterointimenetelmällä. Kuvassa 2 on saman klusterointiongelman dendrogrammi.\n",
    "\n",
    "<br>\n",
    "<div style=\"width:image width px; font-size:80%; text-align:center;\">\n",
    "    <center>\n",
    "    <img src='kuvat\\ohjaamaton1.png' width='550' height='auto' style='padding-bottom:0.5em;' />\n",
    "    </center>\n",
    "    <span>Kuva 1. Erään klusterointiongelman Venn-diagrammit kokoavalla ja jakavalla hierarkkisella klusterointimenetelmällä.</span>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<div style=\"width:image width px; font-size:80%; text-align:center;\">\n",
    "    <center>\n",
    "    <img src='kuvat\\ohjaamaton2.png' width='550' height='auto' style='padding-bottom:0.5em;' />\n",
    "    </center>\n",
    "    <span>Kuva 2. Klusteroinnin lopputuloksesta muodostettu dendrogrammi.</span>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "**Tiheysperusteisissa klusterointimenetelmissä** nimensä mukaisesti datanäytteet jaetaan klustereihin niiden tiheystiedon perusteella. Tunnetuin tiheysperusteinen klusterointimenetelmä on DBSCAN (engl. density-based spatial clustering of applications with noise). DBSCAN-algoritmi ottaa parametreina näytteiden välisen minimietäisyyden ja minimimäärän näytteitä, joista vielä muodostuu klusteri. Yhdessä nämä kaksi parametria määräävät tiheyskriteerin, jonka perusteella DBSCAN yhdistää lähellä toisiaan olevat näytteet yhdeksi klusteriksi. Mikäli tiheyskriteerin perusteella jokin näytteistä on kaukana mistään potentiaalisesta klusterista, jätetään poikkeava havainto (engl. outlier) kokonaan klusteroimatta. DBSCAN-algoritmin hyvä puoli on se, että etukäteen ei tarvitse määrittää klustereiden lukumäärää, kuten monessa muussa klusterointimenetelmässä. Lisäksi algoritmi kykenee löytämään tiheystiedon perusteella minkä tahansa muotoisia klustereita.  DBSCAN on kuitenkin erittäin sensitiivinen määritettyihin parametreihin ja mikäli tiheyskriteeri on valittu väärin, menetelmä tuottaa epäonnistuneen lopputuloksen. DBSCAN ei aina myöskään onnistu klusteroimaan oikein tapauksia, joissa näytteiden välinen tiheys klusterin sisällä vaihtelee merkittävästi.\n",
    "\n",
    "**Keskipisteperusteisessa klusterointimenetelmässä** tai toisin sanoen K-means-algoritmissa mitataan näytteiden samankaltaisuutta vertaamalla niiden etäisyyksiä klusterikeskipisteisiin. Menetelmässä klustereiden lukumäärä tulee tietää etukäteen. Aluksi K-means sijoittaa sattumanvaraisesti klusterikeskipisteet data-avaruuteen. Tämän jälkeen menetelmä iteroi kahden seuraavan vaiheen välillä:\n",
    "\n",
    "1.\tJokainen näyte datassa klusteroidaan sen mukaan, mikä on näytteen lähin klusterikeskipiste euklidisen etäisyyden perusteella.\n",
    "\n",
    "2.\tKlusterikeskipisteet lasketaan uudestaan ottamalla vaiheen 1 mukaisesti klusteroiduista näytteistä keskiarvot ja asetetaan lasketut keskiarvot uusiksi klusterikeskipisteiksi. \n",
    "\n",
    "Algoritmi iteroi näiden kahden vaiheen välillä kunnes ennalta määrätty pysähtymisehto toteutuu. Pysähtymisehto voi olla esimerkiksi etukäteen annetun iteraatioiden määrän saavuttaminen tai tilanne, kun klusterikeskipisteiden sijainnit data-avaruudessa eivät enää päivity iteraatioiden välillä. K-means-algoritmilla suppeneminen lopputulokseen on taattu vaikkakin joskus klusteroinnin lopputulos saattaa epäonnistua.\n",
    "\n",
    "Klusterikeskipisteiden alustaminen on K-means-algoritmin toimivuuden kannalta tärkeässä roolissa. Sattumanvaraisen sijoittelun sijaan yksi hyvä strategia on alustaa klusterikeskipisteet yhtä etäälle toisistaan. Kuvassa 3 on klusteroinnin lopputulos K-means-algoritmilla datalle, jonka näytteet ovat normaalijakautuneissa klustereissa. Jokainen näyte datassa kuuluu lähimmän klusterikeskipisteen määräämään klusteriin.\n",
    "\n",
    "<br>\n",
    "<div style=\"width:image width px; font-size:80%; text-align:center;\">\n",
    "    <center>\n",
    "    <img src='kuvat\\ohjaamaton3.png' width='450' height='auto' style='padding-bottom:0.5em;' />\n",
    "    </center>\n",
    "    <span>Kuva 3. K-means-algoritmin tuottama lopputulos klusterikeskipisteineen.</span>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "Koska klustereiden lukumäärä täytyy tuntea ennen monen eri klusterointimenetelmän käyttämistä, sen arvioimiseen on kehitetty erilaisia mittoja. Yksi käytetty mitta optimaalisen klustereiden lukumäärän selvittämiseksi on siluettikerroin (engl. silhouette score). Siluettikerroin kertoo, kuinka hyvin keskimäärin näytteet sopivat niiden omiin klustereihinsa verrattuna muihin klustereihin.\n",
    "\n",
    "**Malliperusteisissa menetelmissä** data kuvataan mahdollisimman tarkasti jonkin tunnetun matemaattisen mallin avulla. Matemaattisena mallina voidaan käyttää esimerkiksi todennäköisyystiheysjakaumaa tai se voidaan opettaa neuroverkolla.\n",
    "\n",
    "## Dimensionaalisuuden vähentäminen\n",
    "\n",
    "Dimensionaalisuuden vähentämisessä pudotetaan korkeadimensioisen datan piirteiden lukumäärää samalla menettäen mahdollisimman vähän merkittävää informaatiota. Muunnoksen jälkeen datan tärkeimmät ominaisuudet voidaan selittää pienemmällä määrällä alkuperäisestä datasta valituilla tai muunnetuilla piirteillä. Kyseessä on siis datan tehokkaan esitystavan etsiminen.\n",
    "\n",
    "Dimensionaalisuuden vähentämisessä on kaksi erilaista lähestymistapaa: piirteiden valitseminen (engl. feature selection) ja piirteiden irrotus (engl. feature extraction). Piirteiden valitsemisessa keskitytään löytämään datan alkuperäisistä muuttujista ne, jotka mallintavat mahdollisimman hyvin koko datan ominaisuuksia. Piirteiden irrotuksessa puolestaan muunnetaan korkeadimensioinen data matalampidimensioiseksi yhdistämällä toistensa kanssa korreloivia muuttujia uusiksi piirteiksi.\n",
    "\n",
    "Piirteiden irrotusta käytetään datan visualisoimiseen, kun halutaan selvittää ennestään tuntemattoman datan rakenne projisoimalla korkeadimensioinen data kaksi- tai kolmiulotteiseen kuvaajaan. Toinen yleinen käyttötarkoitus piirteiden irrotukseen on pienentää jopa tuhansista muuttujista koostuvan datan piirteiden määrää muutamiin kymmeniin piirteisiin. Näin saadaan pienennettyä ohjatun oppimisen luokittelijan opettamisprosessiin kuluvaa aikaa päästen samalla parempiin luokittelutuloksiin.\n",
    "\n",
    "Eri dimensionaalisuuden vähentämismenetelmät pyrkivät säilyttämään datan eri ominaisuuksia, sillä kaikkia niitä ei voida pitää. Ne voivat esimerkiksi pyrkiä minimoimaan keskineliövirhettä, säilyttämään datanäytteiden välisiä euklidisia etäisyyksiä tai ottamaan huomioon datanäytteiden naapuruusinformaation. Mikään yksittäinen dimensionaalisuuden vähentämismenetelmä ei ole muita menetelmiä selvästi parempi, vaan datasta riippuen ne toimivat eri tavalla.\n",
    "\n",
    "Tarkastellaan seuraavaksi tarkemmin, miten eri dimensionaalisuuden vähentämismenetelmät suorittavat muunnoksen.\n",
    "\n",
    "**Pääkomponenttianalyysi** (engl. principal components analysis, PCA) on tunnetuin lineaarinen dimensionaalisuuden vähentämismenetelmä, josta käytetään myös nimityksiä Hotelling-muunnos tai Karhunen-Loéve-muunnos. Siinä korkeaulotteiseen data-avaruuteen määritellään uudet muuttujat eli ns. pääkomponentit, jotka ovat riippumattomia lineaarikombinaatioita kaikista datan muuttujista.  Pääkomponentit valitaan siten, että ne kuvaavat suurimman osan datan vaihtelusta ja ovat ortogonaalisia toisiinsa nähden. Ensimmäisen pääkomponentin tulee selittää mahdollisimman suuri osa alkuperäisen datan vaihtelusta ja siitä seuraavat pääkomponentit selittävät mahdollisimman hyvin jäljelle jäävän datan vaihtelun.\n",
    "\n",
    "Pääkomponenttianalyysi toimii hyvin, mikäli muuttujien välillä on vahvoja korrelaatioita ja datan riippuvuus on lineaarisesti selitettävissä. Se ei kuitenkaan onnistu muuntamaan hyvin dataa, joka sisältää paljon epälineaarisia riippuvuuksia. Kuvassa 4 kolmiulotteiseen data-avaruuteen on laskettu pääkomponentit $v_{1}$, $v_{2}$ ja $v_{3}$, jonka jälkeen  pääkomponenteista $v_{1}$, $v_{2}$ tulee kaksiulotteisen kuvaajan akselit ja dimensioita saadaan pudotettua yhdellä.\n",
    "\n",
    "<br>\n",
    "<div style=\"width:image width px; font-size:80%; text-align:center;\">\n",
    "    <center>\n",
    "    <img src='kuvat\\ohjaamaton4.png' width='650' height='auto' style='padding-bottom:0.5em;' />\n",
    "    </center>\n",
    "    <span>Kuva 4. Kolmidimensioisen datan muunnos kaksidimensioiseksi pääkomponenttianalyysillä.</span>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "**Monidimensioskaalaus**  (engl. multidimensional scaling, MDS) on joukko menetelmiä, joissa lasketaan aluksi korkeaulotteisessa avaruudessa näytteiden väliset etäisyydet, jonka jälkeen näytteet projisoidaan matalampidimensioiseen avaruuteen siten, että etäisyydet säilyvät muunnoksen jälkeen mahdollisimman ennallaan. Monidimensioskaalaus ottaa datan sisään symmetrisenä etäisyysmatriisina, jossa matriisin elementit kuvaavat näytteiden erilaisuutta/etäisyyttä toisiinsa nähden. Jos näytteiden erilaisuuden mittana käytetään euklidista etäisyyttä, tuottavat monidimensioskaalaus ja pääkomponenttianalyysi saman lopputuloksen. Tällöin monidimensioskaalauksesta käytetään nimitystä klassinen monidimensioskaalaus (engl. classical multidimensional scaling), joka on lineaarinen dimensionaalisuuden vähentämismenetelmä. Usein kuitenkin näytteiden erilaisuutta mitataan jollain muulla etäisyysfunktiolla datan epälineaaristen riippuvuussuhteiden löytämiseksi.\n",
    "\n",
    "Täydellistä sovitusta eri dimensioiden välillä ei voida saavuttaa mutta käyttämällä minimoitavaa häviöfunktiota voidaan päästä hyviin tuloksiin.\n",
    "\n",
    "**Isomap** on epälineaarinen dimensionaalisuuden vähentämismenetelmä, joka säilyttää datanäytteiden väliset geodeettiset etäisyydet eli näytteiden väliset etäisyydet monikerralla (engl. manifold). Geodeettisten etäisyyksien laskemiseksi rakennetaan aluksi naapuruusgraafi liittämällä jokainen datanäyte k-lähimpään naapuriinsa. Seuraavaksi graafissa estimoidaan pisteiden välisiä lyhimpiä etäisyyksiä siihen tarkoitetulla algoritmilla. Lopuksi geodeettiset etäisyydet sijoitetaan etäisyysmatriisiin, joka syötetään klassisen monidimensioskaalaus algoritmin ratkaistavaksi.\n",
    "\n",
    "Koska lasketut geodeettiset etäisyydet ovat vain estimaatteja todellisista geodeettisista etäisyyksistä, saattaa Isomap luoda virheellisiä polkuja naapuruusgraafissa. Mikäli datanäytteet ovat jossain kohtaa monikertaa harvassa, yliarvioi Isomap geodeettisia etäisyyksiä, mikä näkyy liian venyneinä kohtina tuotetussa muunnoksessa.\n",
    "\n",
    "Kuvassa 5 on tehty kolmidimensioisen korkkiruuvinmuotoisen monikerran muunnos kaksidimensioiseksi Isomap-algoritmilla. Vierekkäisiä näytteitä monikerralla on havainnollistettu asteittain muuttuvin sateenkaaren värein. Lopputuloksen perusteella monikerran muoto on onnistuttu säilyttämään hyvin ja kuvaus on onnistunut.\n",
    "\n",
    "<br>\n",
    "<div style=\"width:image width px; font-size:80%; text-align:center;\">\n",
    "    <center>\n",
    "    <img src='kuvat\\ohjaamaton5.png' width='650' height='auto' style='padding-bottom:0.5em;' />\n",
    "    </center>\n",
    "    <span>Kuva 5. Kolmedimensioisen korkkiruuvinmuotoisen monikerran muunnos kaksidimensioiseksi Isomap-algoritmilla.</span>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "**t-SNE** (engl. t-distributed stochastic neighbor embedding) on vuonna 2008 julkaistu epälineaarinen datan visualisointi ja dimensionaalisuuden vähentämismenetelmä. Se pyrkii säilyttämään datan lokaalin rakenteen eli samanlaiset näytteet alkuperäisessä data-avaruudessa ovat muunnoksen jälkeen mahdollisimman lähellä toisiaan. Toisaalta t-SNE pyrkii säilyttämään myös datan globaalin rakenteen eli erilaiset klusterit alkuperäisessä data-avaruudessa ovat muunnoksen jälkeen mahdollisimman erillään toisistaan.\n",
    "\n",
    "t-SNE-algoritmi muodostaa aluksi todennäköisyystiheysjakauman jokaiselle näytteelle erikseen korkeassa ulottuvuudessa siten, että samanlaiset naapurit saavat mahdollisimman suuren arvon ja erilaiset naapurit mahdollisimman pienen. Seuraavaksi t-SNE-algoritmi määrittää korkean ulottuvuuden kanssa samankaltaiset todennäköisyystiheysjakaumat matalassa ulottuvuudessa sattumanvaraisesti sijoitetuille näytteille. Lopuksi algoritmi etsii näytteille optimaaliset paikat matalassa ulottuvuudessa minimoiden iteratiivisesti Kullback-Leibler divergenssin. Päämääränä on, että näytteiden todennäköisyystiheysjakaumat vastaavat mahdollisimman hyvin toisiaan ulottuvuuksien välillä. Yleisimmin algoritmissa käytetään näytteiden samankaltaisuuden mittana euklidista etäisyyttä, mutta sen sijasta voidaan käyttää myös muita painotettuja etäisyyden mittoja.\n",
    "\n",
    "Kuvassa 6 on algoritmien PCA, MDS, Isomap ja t-SNE tuottamat lopputulokset MNIST-datalle. Kuvasta huomataan, kuinka epälineaariset dimensionaalisuuden vähentämismenetelmät t-SNE, monidimensioskaalaus ja Isomap onnistuvat erottelemaan ryhmiä paremmin kuin lineaarinen dimensionaalisuuden vähentämismenetelmä pääkomponenttianalyysi.\n",
    "\n",
    "<br>\n",
    "<div style=\"width:image width px; font-size:80%; text-align:center;\">\n",
    "    <center>\n",
    "    <img src='kuvat\\ohjaamaton6.png' width='900' height='auto' style='padding-bottom:0.5em;' />\n",
    "    </center>\n",
    "    <span>Kuva 6. MNIST-datan dimensionaalisuuden vähentäminen algoritmeilla PCA, MDS, Isomap ja t-SNE.</span>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "\n",
    "## Lähteet\n",
    "\n",
    "[1] Duda R., Hart P. & Stork D. (2012) Pattern classification. John Wiley & Sons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
