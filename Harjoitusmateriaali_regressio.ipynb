{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from onedrivedownloader import download\n",
    "\n",
    "link2data = 'https://unioulu-my.sharepoint.com/:u:/g/personal/jukmaatt_univ_yo_oulu_fi/EfyEmGc8QYtCqA_H6_Am0QwBZKTra-JVQaPdN5vA4E_TRg?e=GVnvV3'\n",
    "\n",
    "if not os.path.exists('./kuvat'):\n",
    "    print('Downloading data')\n",
    "    download(link2data, filename='./tiedostot.zip', unzip=True, unzip_path='./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>521160P Johdatus Tekoälyyn<br><br>Harjoitusmateriaali<br><br>Regressio<br>\n",
    "\n",
    "Tekoälyn osa-alue koneoppiminen jaetaan ohjattuun oppimiseen, ohjaamattomaan oppimiseen ja vahvistusoppimiseen. Ohjatussa oppimisessa malli opetetaan datalla, jolle on tarjolla datanäytteiden lisäksi lähtömuuttujat. Opetuksen jälkeen malli kykenee ennustamaan, minkä lähtömuuttujan uusi tuntematon datanäyte saa. Ohjaamattomassa oppimisessa malli opetetaan ilman lähtömuuttujia sisältävän datan perusteella, jolloin opetettava malli oppii datan rakenteisuuden. Vahvistusoppimisessa tarjolla ei ole dataa eikä lähtömuuttujia. Siinä toimintaympäristössään toimiva agentti saa tekemistään liikkeistä reaaliaikaisesti positiivisia ja negatiivisia palautteita, joista se oppii yritysten ja erehdysten kautta löytämään optimaalisen strategian.\n",
    "\n",
    "Ohjattu oppiminen jaetaan edelleen luokitteluun ja regressioon. Regressio on tilastollinen menetelmä, joka arvioi datan tulomuuttujien $X$ = $[x_{1}, x_{2}, ..., x_{n}]$ ja lähtömuuttujan $Y$ välistä riippuvuutta. Regressiossa tulomuuttujien arvot voivat olla joko jatkuva-arvoisia tai diskreettejä mutta lähtömuuttujan arvo on aina jatkuva-arvoinen. Yksinkertaisin regressio-ongelma käsittelee yhtä selitettävää muuttujaa (tai riippuvaa muuttujaa) $Y$, joka riippuu ainoastaan yhdestä selittävästä muuttujasta (tai riippumattomasta muuttujasta) $X$. Tehtävänä on löytää tälle tilastolliselle ongelmalle malli, joka kuvaa parhaiten käytettävissä olevan datan perusteella $X$:n ja $Y$:n välistä riippuvuutta. Regressio-analyysin peruskysymykset ovat: Mitä matemaattista mallia tulisi käyttää ongelman ratkaisussa (suora, paraabeli, logaritmifunktio jne.) sekä kuinka sovitamme parhaiten sopivan mallin kuvaajaan?\n",
    "\n",
    "## Regressioanalyysi\n",
    "\n",
    "Lineaarisessa regressiossa käytetään mallina yhtälön (1) ensimmäisen asteen polynomifunktiota eli suoraa. Tarkoituksena on löytää parametreille k ja b optimaaliset arvot datan perusteella.\n",
    "\n",
    "\\begin{equation}\n",
    "y = kx + b\\:, \\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "*missä k on kulmakerroin ja b on y-akselin leikkauskohta.*\n",
    "\n",
    "\n",
    "Joissain tapauksissa korkeamman asteen polynomifunktio kuvaa paremmin datan ominaisuuksia kuin\n",
    "lineaarinen suora. Yksinkertaisin laajennus suoran yhtälöstä on toisen asteen polynomifunktio eli paraabeli. Malliksi saadaan korkeamman asteen polynomifunktioita lisäämällä termejä $x^2$, $x^3$, $...$ $x^n$ yhtälön (1) lineaarisen regression malliin yhtälön (2) mukaisesti.\n",
    "\n",
    "\\begin{equation}\n",
    "y = a_{0} + a_{1}x + a_{2}x^2 + ... + a_{k}x^k \\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "Jos data sallii käyrän piirtämisen kaikkien datapisteiden läpi, parhaiten sopivan mallin löytämisessä ei tule olemaan ongelmaa. Tosielämässä kuitenkin esimerkiksi saman vuosimallin autoilla ei ole tapana olla täsmälleen saman hintaisia. Tästä syystä jokaista yksittäistä y-koordinaatin arvoa ei voida ennustaa täydellisesti x-koordinaatin arvoista ja ennuste sisältää virhetermin eli residuaalin.\n",
    "\n",
    "Matemaattisen mallin sovittaminen datapisteisiin on pitkään pohdittu ongelma ja sen ratkaisemiseksi käytetään yleensä pienimmän neliösumman menetelmää (engl. least squares method). Pienimmän neliösumman menetelmä käyttää L2-normalisointia, joka etsii optimaaliset parametrit sovitettavalle mallille minimoimalla pystysuuntaisten virhetermien neliöiden summan. Toinen vaihtoehtoinen normalisointitapa on L1-normalisointi, jossa lasketaan pystysuuntaisten virhetermien itseisarvoistettu summa. Kaksiulotteisessa tapauksessa [L1-normi](https://mathworld.wolfram.com/L1-Norm.html) (manhattan-normi) ja [L2-normi](https://mathworld.wolfram.com/L2-Norm.html) (euklidinen-normi) lasketaan yhtälöillä (3) ja (4).\n",
    "\n",
    "\\begin{equation}\n",
    "L1_{normi} = \\sum\\limits_{i=1}^n | Y_{i} - f(X_{i}) | \\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "L2_{normi} = \\sqrt{\\sum\\limits_{i=1}^n ( Y_{i} - f(X_{i}) ) ^2} \\:, \\tag{4}\n",
    "\\end{equation}\n",
    "\n",
    "*missä $Y_{i}$ on datanäytteen y-koordinaatin arvo ja $f(X_{i})$ on sovitetulla funktiolla ennustettu y-koordinaatin arvo.*\n",
    "\n",
    "Mitä pienempi virhetermien neliöity tai itseisarvoistettu summa on, sitä lähempänä sovitettu funktio on datanäytteitä. Kuvassa 1 on sovitettu dataan lineaarinen suora pienimmän neliösumman menetelmällä. Kuvaajan mustat pisteet ovat datanäytteitä, punaiset viivat ovat pystysuuntaisia virhetermejä ja sininen viiva on sovitettu suora.\n",
    "\n",
    "<br>\n",
    "<div style=\"width:image width px; font-size:80%; text-align:center;\">\n",
    "    <center>\n",
    "    <img src='kuvat/regressio1.png' width='550' height='auto' style='padding-bottom:0.5em;' />\n",
    "    </center>\n",
    "    <span>Kuva 1. Pienimmän neliösumman menetelmällä sovitettu suora minimoiden neliöityjen virhetermien summa.</span>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "Sovitettavan funktion parametrit pienimmän neliösumman menetelmällä lasketaan matriisialgebralla yhtälön (5) mukaisesti.\n",
    "\n",
    "\\begin{equation}\n",
    "Y = XA \\Leftrightarrow A = (X^TX)^{-1}X^TY\\:, \\tag{5}\n",
    "\\end{equation}\n",
    "\n",
    "*missä $X$ on datan x-koordinaatin arvot matriisissa, $Y$ on datan y-koordinaatin arvot matriisissa ja $A$ on kerroinmatriisi viitaten parametreihin [$a_{0},a_{1},a_{2},…,a_{k}$].*\n",
    "\n",
    "Sovitetun mallin suorituskyvyn mittaamiseen käytetään korrelaatiokertoimen neliötä $r^2$ ja keskineliövirhettä (engl. mean squared error, MSE). Käytettävä tilastollinen mitta kertoo, kuinka hyvin datanäytteet ja sovitettu malli vastaavat toisiaan. Kun korrelaatiokertoimen neliö on 1, niin testidatan muuttujien välillä on täydellinen riippuvuus ja ne saadaan ennustettua täysin oikein mallilla. Korrelaatiokertoimen neliön ollessa 0, muuttujat eivät riipu ollenkaan toisistaan, eikä sovitettu malli vastaa niiden välistä riippuvuutta. \n",
    "\n",
    "Keskineliövirhe puolestaan saa suuria arvoja, kun datanäytteet poikkeavat paljon sovitetusta mallista. Se on 0, kun testidatan muuttujien arvot saadaan ennustettua täydellisesti mallilla. Korrelaatiokertoimen neliö sovitetulle mallille lasketaan yhtälöiden (6), (7), (8), (9) ja (10) avulla. Keskineliövirhe sovitetulle mallille lasketaan yhtälöllä (11).\n",
    "\n",
    "\\begin{equation}\n",
    "SSE = \\sum\\limits_{i=1}^n (Y_{i} - f(X_{i}))^2 \\tag{6}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "SST = \\sum\\limits_{i=1}^n (Y_{i} - \\bar{Y})^2 \\tag{7}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "SSR = \\sum\\limits_{i=1}^n (f(X_{i}) - \\bar{Y})^2 \\tag{8}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "SST = SSE + SSR \\tag{9}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "r^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST} = 1 - \\frac{\\sum\\limits_{i=1}^n (Y_{i} - f(X_{i}))^2}{\\sum\\limits_{i=1}^n (Y_{i} - \\bar{Y})^2} \\tag{10}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "MSE = \\frac{1}{n} \\sum\\limits_{i=1}^n (Y_{i} - f(X_{i}))^2\\:, \\tag{11}\n",
    "\\end{equation}\n",
    "\n",
    "*missä SSE (engl. the sum of squared errors) on neliöityjen virhetermien summa. SST (engl. the total sum of squares) on datanäytteiden y-koordinaatin arvojen ja niiden keskiarvon välisten neliöityjen erotuksien summa. SSR (engl. sum of squares due to regression) ennustettujen y-koordinaatin arvojen ja datanäytteiden y-koordinaatin arvojen keskiarvon välisten neliöityjen erotuksien summa*.\n",
    "\n",
    "Esimerkiksi käytetään mallina toisen asteen yhtälöä, joka on muotoa $y=a_{0}+a_{1}x+a_{2}x^2$ ja sovitetaan malli pisteille (2,5), (-2,5), (0,0) ja (0,2). Tällöin matriisit $X$,$Y$ ja $A$ saavat muodon:\n",
    "\n",
    "\\begin{equation*}\n",
    "X = \\begin{bmatrix}\n",
    "1 & 2 & 4\\\\ \n",
    "1 & -2 & 4\\\\ \n",
    "1 & 0 & 0\\\\ \n",
    "1 & 0 & 0\n",
    "\\end{bmatrix},\n",
    "\\:\\:\n",
    "Y = \\begin{bmatrix}\n",
    "5\\\\ \n",
    "5\\\\ \n",
    "0\\\\ \n",
    "2\n",
    "\\end{bmatrix},\n",
    "\\:\\:\n",
    "A = \\begin{bmatrix}\n",
    "a_{0}\\\\ \n",
    "a_{1}\\\\ \n",
    "a_{2}\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Käyttämällä yhtälöä (5) saadaan laskettua:\n",
    "\n",
    "\\begin{equation*}\n",
    "A = (X^TX)^{-1}X^TY\n",
    "= \\begin{bmatrix}\n",
    "0.5 & 0 & -0.125\\\\ \n",
    "0 & 0.125 & 0\\\\ \n",
    "-0.125 & 0 & 0.625\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 & 1\\\\ \n",
    "2 & -2 & 0 & 0\\\\ \n",
    "4 & 4 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "5\\\\ \n",
    "5\\\\ \n",
    "0\\\\ \n",
    "2\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "1\\\\ \n",
    "0\\\\ \n",
    "1\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Eli kerroinmatriisiksi A saadaan ratkaistua $A = [1\\:0\\:1]^T$, joten toisen asteen yhtälöksi tulee $y = 1 + 0\\cdot x + 1\\cdot x^2$.\n",
    "Lasketaan vielä sovitetun mallin korrelaatiokertoimen neliö ja keskineliövirhe opetusdatalle yhtälöillä (10) ja (11):\n",
    "\n",
    "\\begin{equation*}\n",
    "r^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{\\sum\\limits_{i=1}^n (Y_{i} - f(X_{i}))^2}{\\sum\\limits_{i=1}^n (Y_{i} - \\bar{Y})^2} = 1 - \\frac{(5-5)^2 + (5-5)^2 + (0-1)^2 + (2-1)^2}{(5-3)^2 + (5-3)^2 + (0-3)^2 + (2-3)^2} \\approx 0,889\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "MSE = \\frac{1}{n} \\sum\\limits_{i=1}^n (Y_{i} - f(X_{i}))^2 = \\frac{(5-5)^2 + (5-5)^2 + (0-1)^2 + (2-1)^2}{4} = 0,5\n",
    "\\end{equation*}\n",
    "\n",
    "Ylioppiminen on yleinen ongelma koneoppimisessa ja erityisesti ohjatussa oppimisessa. Jos sovitettava malli on liian monimutkainen, kuten liian korkea-asteinen polynomifunktio, alkaa se mallintamaan datassa esiintyvää kohinaa eli värähtelyä ja tapahtuu ylioppiminen. Näin on tapahtunut kuvan 2 vasemmanpuoleisessa kuvaajassa. Vaikka malli kulkee lähes kaikkien datapisteiden kautta ja opetusdatan korrelaatiokerroin on ~1 ja keskineliövirhe ~0, ei se siltikään ole paras mahdollinen malli mallintamaan muuttujien välistä riippuvuutta. \n",
    "\n",
    "Jos taas kompleksiselle datalle valitaan liian yksinkertainen malli, ei se onnistu jäljittelemään datan rakenteisuutta riittävän tarkasti ja kyseessä on alioppiminen. Näin on tapahtunut kuvan 2 oikeanpuoleisessa kuvaajassa. Sopivassa sovituksessa vältetään nämä molemmat ei-toivotut ilmiöt.\n",
    "\n",
    "<br>\n",
    "<div style=\"width:image width px; font-size:80%; text-align:center;\">\n",
    "    <center>\n",
    "    <img src='kuvat/regressio2.png' width='1100' height='auto' style='padding-bottom:0.5em;' />\n",
    "    </center>\n",
    "    <span>Kuva 2. Ylioppinut malli, sopivasti sovitettu malli ja alioppinut malli regressio-ongelmalle.</span>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "## Logistinen regressio\n",
    "\n",
    "Logistinen regressio on regressioanalyysin erityistapaus, joka pyrkii ennustamaan, millä todennäköisyydellä määritelty tapahtuma tulee tapahtumaan. Se on epälineaarinen regressiomenetelmä, jota käytetään yleensä nimestään huolimatta datan luokitteluun. Opetusdatan näytteillä tulee siis olla tieto luokista, joihin ne kuuluvat. Mikäli luokiteltavia luokkia on vain kaksi, käytetään binääristä logistista regressiota, jonka logistinen sigmoid-funktio lasketaan yhtälöllä (12).\n",
    "\n",
    "\\begin{equation}\n",
    "P(Y=1) = \\frac{1}{1+e^{-t}}\\:, \\tag{12}\n",
    "\\end{equation}\n",
    "\n",
    "*missä $t = a_{0} + a_{1}x + ⋯ + a_{k}x^k$ ja $x$ on selittävä muuttuja.*\n",
    "\n",
    "Kahden luokan tapauksessa tapahtuman $P(Y=1)$ vastatapahtuma saadaan laskettua $P(Y=0) = 1\\:–\\:P(Y=1)$. Mikäli luokkia on enemmän kuin kaksi, käytetään multinomiaalista logistista regressiota, jonka logistinen softmax-funktio lasketaan yhtälöllä (13).\n",
    "\n",
    "\\begin{equation}\n",
    "P(Y=i) = \\frac{e^{\\:t_i}}{\\sum\\limits_{n=1}^N e^{\\:t_n}}\\:, \\tag{13}\n",
    "\\end{equation}\n",
    "\n",
    "*missä $t_i = a_{0i} + a_{1i}x + ⋯ + a_{ki}x^k$, $x$ on selittävä muuttuja ja N on luokkien lukumäärä.*\n",
    "\n",
    "Tutkitaan esimerkkinä, miten erään kasvin siemenen koko vaikuttaa sen itämistodennäköisyyteen. Kyseessä on kahden luokan tapaus, sillä siemen joko itää tai jää itämättä. Taulukossa 1 on esitetty 25 siemennäytteestä koostuva data, jossa 10 siemenistä iti ja 15 jäi itämättä. [1]\n",
    "\n",
    "<br>*Taulukko 1. Siemendata*\n",
    "\n",
    "| <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| **Siemenen koko (mm)** | 13,8 | 14,8 | 16,3 | 17,0 | 17,3 | 18,2 | 18,3 | 18,9 | 19,3 | 19,6 | 19,9 | 20,3 | 20,4 | 21,1 | 21,2 | 21,3 | 21,4 | 21,6 | 21,8 | 23,2 | 24,3 | 24,9 | 25,1 | 26,0 | 27,6 |\n",
    "| **Siemen iti? (k/e)** | e | e | e | e | e | e | e | k | e | e | k | e | e | e | e | k | k | e | k | e | k | k | k | k | k |\n",
    "<br>\n",
    "\n",
    "\n",
    "Opettamalla logistisen regression luokittelija saadaan logistiseksi funktioksi $P(Y=1) = \\frac{1}{1 + e^{-0,65x+14}}$. Nyt voidaan arvioida esimerkiksi, millä itämistodennäköisyydellä 21,0 mm kokoinen siemen tulee itämään. Kuvassa 3 on piirretty kuvaajaan opittu logistinen funktio ja taulukon 1 näytteet. Kuvaajassa itämistodennäköisyyden arvo 1.0 tarkoittaa, että siemen itää 100 % todennäköisyydellä ja 0.0, että siemen itää 0 % todennäköisyydellä. Kuvaajasta nähdään, että logistisen regression mallin perusteella 21,0 mm kokoinen siemen tulee itämään noin 40 % todennäköisyydellä.\n",
    "\n",
    "<br>\n",
    "<div style=\"width:image width px; font-size:80%; text-align:center;\">\n",
    "    <center>\n",
    "    <img src='kuvat/regressio3.png' width='600' height='auto' style='padding-bottom:0.5em;' />\n",
    "    </center>\n",
    "    <span>Kuva 3. Siemenen koko - itämistodennäköisyys kuvaaja.</span>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "## Lähteet\n",
    "\n",
    "[1] Vanhoenacker D. Logistic Regression. URL:http://hem.bredband.net/didrik71/recostat/logreg_e.htm.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
