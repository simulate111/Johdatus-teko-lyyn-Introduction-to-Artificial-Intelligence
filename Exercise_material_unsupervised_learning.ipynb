{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simulate111/Johdatus-teko-lyyn-Introduction-to-Artificial-Intelligence/blob/main/Exercise_material_unsupervised_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install onedrivedownloader"
      ],
      "metadata": {
        "id": "p6goAFYHPIxz",
        "outputId": "ce224f7a-543f-435d-e306-c7f05f9d21ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onedrivedownloader\n",
            "  Downloading onedrivedownloader-1.1.3-py3-none-any.whl (5.1 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from onedrivedownloader) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from onedrivedownloader) (4.66.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->onedrivedownloader) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->onedrivedownloader) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->onedrivedownloader) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->onedrivedownloader) (2024.2.2)\n",
            "Installing collected packages: onedrivedownloader\n",
            "Successfully installed onedrivedownloader-1.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "adykQq05OzAp",
        "outputId": "7e64e559-5114-4a62-e649-e0e74aad1ac4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.20M/4.20M [00:01<00:00, 2.27MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unzipping file...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting files: 100%|██████████| 18/18 [00:00<00:00, 256.12it/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "from onedrivedownloader import download\n",
        "\n",
        "link2data = 'https://unioulu-my.sharepoint.com/:u:/g/personal/jukmaatt_univ_yo_oulu_fi/EVQKkc2thRdAptI6z4eitF4BCQsEPrv6ocioVkoyeQFf3A?e=qBQ6ry'\n",
        "\n",
        "if not os.path.exists('./data'):\n",
        "    print('Downloading data')\n",
        "    download(link2data, filename='./files.zip', unzip=True, unzip_path='./')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ee2NoaEOzAq"
      },
      "source": [
        "# <center>521160P Introduction to artificial intelligence<br><br>Exercise material<br><br>Unsupervised learning<br></center>\n",
        "\n",
        "\n",
        "When a machine learning algorithm uses data without output variables, it is unsupervised learning. Because there are no predefined categories for data samples, the grouping of data must be done entirely based on its structure. Unsupervised learning is divided into clustering and dimensional reduction.\n",
        "\n",
        "## Clustering\n",
        "\n",
        "Clustering is a component of unsupervised learning, which is used to analyze the data and its grouping. The goal of clustering is to divide the data into smaller groups, i.e. clusters whose samples are as similar as possible to each other, but the samples of different clusters are as different as possible from each other. The similarity of data samples can be measured, for example, by Euclidean distance or probability densities. Each different clustering problem has its own characteristics, which is why there is no commonly used method that works well in all situations. Indeed, different clustering methods can be roughly divided according to what properties they utilize.\n",
        "\n",
        "**Hierarchical clustering** methods are either divisive or agglomerative [1]. Dividing methods start with a set from which all samples belong. Based on the similarities, the sample set is always divided into smaller subsets step by step until all samples have been clustered. Collecting methods, on the other hand, start with individual samples, which aim to be combined in stages into larger and similar entities. Finally, a simple tree diagram, or dendrogram, can be drawn that hierarchically depicts the similarities between the samples. In Figure 1, a hierarchical clustering method that compiles and divides Venn diagrams is formed for a clustering problem. Figure 2 shows a dendrogram of the same clustering problem.\n",
        "\n",
        "<br>\n",
        "<div style=\"width:image width px; font-size:80%; text-align:center;\">\n",
        "    <center>\n",
        "    <img src='https://github.com/simulate111/Johdatus-teko-lyyn-Introduction-to-Artificial-Intelligence/blob/main/imgs/ohjaamaton1.png?raw=1' width='550' height='auto' style='padding-bottom:0.5em;' />\n",
        "    </center>\n",
        "    <span>Figure 1. Venn-diagrams of agglomerative and divisive hierarchical clustering for one clustering problem.</span>\n",
        "</div>\n",
        "<br>\n",
        "\n",
        "<br>\n",
        "<div style=\"width:image width px; font-size:80%; text-align:center;\">\n",
        "    <center>\n",
        "    <img src='https://github.com/simulate111/Johdatus-teko-lyyn-Introduction-to-Artificial-Intelligence/blob/main/imgs/ohjaamaton2.png?raw=1' width='550' height='auto' style='padding-bottom:0.5em;' />\n",
        "    </center>\n",
        "    <span>Figure 2. Dendrogram formed from the result of clustering.</span>\n",
        "</div>\n",
        "<br>\n",
        "\n",
        "In **density-based clustering** methods, as the name implies, data samples are divided into clusters based on their density information. The best known density-based clustering method is DBSCAN (density-based spatial clustering of applications with noise). The DBSCAN algorithm takes as parameters the minimum distance between samples and the minimum number of samples that still form a cluster. Together, these two parameters determine the density criterion by which DBSCAN combines closely related samples into a single cluster. If, based on the density criterion, one of the samples is far from any potential cluster, the outlier is not clustered at all. The good thing about the DBSCAN algorithm is that there is no need to specify the number of clusters in advance, as with many other clustering methods. In addition, the algorithm is able to find clusters of any shape based on density information. However, DBSCAN is very sensitive to the specified parameters, and if the density criterion is selected incorrectly, the method produces a failed result. DBSCAN also does not always succeed in properly clustering cases where the density between samples within a cluster varies significantly.\n",
        "\n",
        "**In the center-based clustering method** or in other words, the K-means algorithm measures the similarity of the samples by comparing their distances to the cluster centers. In the method, the number of clusters must be known in advance. Initially, K-means randomly places cluster centers in the data space. The method then iterates between the following two steps:\n",
        "\n",
        "1. Each sample in the data is clustered according to which is the nearest cluster center of the sample based on the Euclidean distance.\n",
        "\n",
        "2. The cluster centers are recalculated by averaging the samples clustered in accordance with step 1 and setting the calculated averages as new cluster centers.  \n",
        "\n",
        "The algorithm iterates between these two phases until a predetermined stop condition is met. The stopping condition may be for example reaching a predetermined number of iterations or a situation where the locations of the cluster centers in the data space are no longer updated between the iterations. With the K-means algorithm convergence to the end result is guaranteed although sometimes the end result of clustering may fail.\n",
        "\n",
        "Initialization of cluster centers plays an important role in the functionality of the K-means algorithm. Instead of random placement, one good strategy is to initialize the cluster centers equally far apart. Figure 3 shows the result of clustering with the K-means algorithm for data whose samples are in normally distributed clusters. Each sample in the data belongs to the cluster determined by the nearest cluster center.\n",
        "\n",
        "<br>\n",
        "<div style=\"width:image width px; font-size:80%; text-align:center;\">\n",
        "    <center>\n",
        "    <img src='https://github.com/simulate111/Johdatus-teko-lyyn-Introduction-to-Artificial-Intelligence/blob/main/imgs/ohjaamaton3.png?raw=1' width='450' height='auto' style='padding-bottom:0.5em;' />\n",
        "    </center>\n",
        "    <span>Figure 3. The result produced by the K-means algorithm with its cluster centers. </span>\n",
        "</div>\n",
        "<br>\n",
        "\n",
        "Because the number of clusters must be known before using many different clustering methods different dimensions have been developed to evaluate it. One measure used to determine the optimal number of clusters is the silhouette score. The silhouette coefficient tells you how well on average the samples fit into their own clusters compared to other clusters.\n",
        "\n",
        "**In model-based methods**, data is described as accurately as possible using some known mathematical model. For example, a probability density distribution can be used as a mathematical model or it can be taught with a neural network.\n",
        "\n",
        "## Reducing dimensionality\n",
        "\n",
        "Reducing dimensionality reduces the number of features of high-dimensional data while losing as little significant information as possible. After transformation, the main properties of the data can be explained by a smaller number of features selected or modified from the original data. It is therefore a matter of finding an efficient way of presenting the data.\n",
        "\n",
        "There are two different approaches to reducing dimensionality: feature selection and feature extraction. The selection of features focuses on finding from the original variables of the data those that best model the properties of the entire data. Feature extraction in turn converts high-dimensional data to lower-dimensional by combining correlated variables into new features.\n",
        "\n",
        "Feature extraction is used to visualize data when it is desired to elucidate the structure of previously unknown data by projecting high-dimensional data onto a two- or three-dimensional graph. Another common use for feature removal is to reduce the number of features in data consisting of up to thousands of variables to a few dozen features. This reduces the time spent in the teaching process of the guided learning classifier while achieving better grading results.\n",
        "\n",
        "Different dimensional reduction methods tend to preserve different properties of the data, as not all of them can be considered. For example, they may seek to minimize the mean square error, maintain Euclidean distances between data samples, or take into account the neighborhood information of the data samples. No single dimensional reduction method is clearly superior to other methods but depending on the data they work differently.\n",
        "\n",
        "Next, let’s take a closer look at how different dimensional reduction methods perform the transformation.\n",
        "\n",
        "**Principal components analysis** (PCA) is the best known linear dimensional reduction method, also known as the Hotelling transform or the Karhunen-Loéve transform. It defines new variables for high-dimensional data space, the so-called principal components that are independent linear combinations of all data variables. The main components are selected so that they describe most of the variation in the data and are orthogonal to each other. The first main component should explain as much of the variation in the original data as possible, and the subsequent main components should explain as much as possible the variation in the remaining data.\n",
        "\n",
        "Principal component analysis works well if there are strong correlations between the variables and the dependence of the data can be explained linearly. However, it fails to convert well data that contains a lot of nonlinear dependencies. In Figure 4, the principal components $v_ {1}$, $v_{2}$ and $v_{3}$ are computed into the three-dimensional data space, after which the principal components $v_ {1}$, $v_{2}$ become the axes of the two-dimensional graph. and the dimensions can be dropped by one.\n",
        "\n",
        "<br>\n",
        "<div style=\"width:image width px; font-size:80%; text-align:center;\">\n",
        "    <center>\n",
        "    <img src='https://github.com/simulate111/Johdatus-teko-lyyn-Introduction-to-Artificial-Intelligence/blob/main/imgs/ohjaamaton4.png?raw=1' width='650' height='auto' style='padding-bottom:0.5em;' />\n",
        "    </center>\n",
        "    <span>Figure 4. Conversion of three-dimensional data to two-dimensional by principal component analysis.</span>\n",
        "</div>\n",
        "<br>\n",
        "\n",
        "**Multidimensional scaling** (MDS) is a set of methods that first calculates the distances between samples in high-dimensional space, after which the samples are projected into lower-dimensional space so that the distances remain as unchanged as possible after conversion. Multidimensional scaling incorporates data as a symmetric distance matrix, where the elements of the matrix describe the difference / distance of the samples relative to each other. If Euclidean distance is used as a measure of sample difference, multidimensional scaling and principal component analysis produce the same result. In this case, multidimensional scaling is called Classical multidimensional scaling, which is a linear method of reducing dimensionality. Often, however, the difference in samples is measured by some other distance function to find nonlinear dependencies of the data.\n",
        "\n",
        "A perfect fit between the different dimensions cannot be achieved but good results can be obtained by using a minimized loss function.\n",
        "\n",
        "**Isomap** is a nonlinear dimensionality reduction method that preserves the geodetic distances between data samples, ie the distances between samples are manifold. To calculate geodetic distances, a neighborhood graph is first constructed by appending each data sample to its k-nearest neighbor. Next, the shortest distances between points are estimated in the graph using a dedicated algorithm. Finally, the geodetic distances are placed in a distance matrix that is fed to be solved by a classical multidimensional scaling algorithm.\n",
        "\n",
        "Because the calculated geodetic distances are only Estimates of the actual geodetic distances, Isomap may create incorrect paths in the neighborhood graph. If the data samples are somewhere multiple times sparse, Isomap overestimates the geodetic distances, which is reflected in the transformation produced as too stretched points.\n",
        "\n",
        "Figure 5 shows the conversion of a three-dimensional corkscrew multiple to a two-dimensional one using the Isomap algorithm. Adjacent samples on multiple occasions have been illustrated with progressively changing rainbow colors. Based on the end result, the multiple form has been well preserved and the description has been successful.\n",
        "\n",
        "<br>\n",
        "<div style=\"width:image width px; font-size:80%; text-align:center;\">\n",
        "    <center>\n",
        "    <img src='https://github.com/simulate111/Johdatus-teko-lyyn-Introduction-to-Artificial-Intelligence/blob/main/imgs/ohjaamaton5.png?raw=1' width='650' height='auto' style='padding-bottom:0.5em;' />\n",
        "    </center>\n",
        "    <span>Figure 5. Three-dimensional conversion of a three-dimensional corkscrew to two-dimensional via the Isomap algorithm.</span>\n",
        "</div>\n",
        "<br>\n",
        "\n",
        "**t-SNE** (t-distributed stochastic neighbor embedding) is a nonlinear data visualization and dimensionality reduction method published in 2008. It tends to preserve the local structure of the data, i.e., similar samples in the original data space are as close to each other as possible after the transformation. On the other hand, the t-SNE also tends to preserve the global structure of the data, i.e. the different clusters in the original data space are as separate as possible after the transformation.\n",
        "\n",
        "The t-SNE algorithm initially forms a probability density distribution for each sample individually in a high dimension so that similar neighbors get the highest possible value and different neighbors get the smallest possible value. Next, the t-SNE algorithm determines probability density distributions similar to the high dimension for randomly placed samples in the low dimension. Finally, the algorithm searches for optimal locations for the samples in the low dimension, iteratively minimizing the Kullback-Leibler divergence. The goal is for the probability density distributions of the samples to correspond as closely as possible between the dimensions. Most commonly, the algorithm uses Euclidean distance as a measure of sample similarity, but other weighted distance measures can be used instead.\n",
        "\n",
        "Figure 6 shows the results for MNIST data produced by the algorithms PCA, MDS, Isomap, and t-SNE. The figure shows how the nonlinear dimensional reduction methods t-SNE, multidimensional scaling, and Isomap succeed in distinguishing groups better than the linear dimensional reduction method principal component analysis.\n",
        "\n",
        "<br>\n",
        "<div style=\"width:image width px; font-size:80%; text-align:center;\">\n",
        "    <center>\n",
        "    <img src='https://github.com/simulate111/Johdatus-teko-lyyn-Introduction-to-Artificial-Intelligence/blob/main/imgs/ohjaamaton6.png?raw=1' width='900' height='auto' style='padding-bottom:0.5em;' />\n",
        "    </center>\n",
        "    <span>Figure 6. Reducing the dimensionality of MNIST data by PCA, MDS, Isomap, and t-SNE algorithms.</span>\n",
        "</div>\n",
        "<br>\n",
        "\n",
        "\n",
        "## Sources\n",
        "\n",
        "[1] Duda R., Hart P. & Stork D. (2012) Pattern classification. John Wiley & Sons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IbNyUmMxOzAs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}