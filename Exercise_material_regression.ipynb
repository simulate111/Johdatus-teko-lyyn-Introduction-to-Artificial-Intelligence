{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simulate111/Johdatus-teko-lyyn-Introduction-to-Artificial-Intelligence/blob/main/Exercise_material_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install imblearn umap-learn onedrivedownloader"
      ],
      "metadata": {
        "id": "nb-Pnx1IOpOP",
        "outputId": "f87b7fab-2e17-48f2-9745-5b778a2aefcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imblearn in /usr/local/lib/python3.10/dist-packages (0.0)\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.10/dist-packages (0.5.5)\n",
            "Requirement already satisfied: onedrivedownloader in /usr/local/lib/python3.10/dist-packages (1.1.3)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (from imblearn) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.2.2)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.58.1)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.5.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.66.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from onedrivedownloader) (2.31.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.41.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->onedrivedownloader) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->onedrivedownloader) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->onedrivedownloader) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->onedrivedownloader) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nASaQxksOZdv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from onedrivedownloader import download\n",
        "\n",
        "link2data = 'https://unioulu-my.sharepoint.com/:u:/g/personal/jukmaatt_univ_yo_oulu_fi/EccjaL-zb_JKiZA_OBR3-VsBOa69ZTuk-yDw7j7hsKqaFw?e=ZqIJk0'\n",
        "\n",
        "if not os.path.exists('./imgs'):\n",
        "    print('Downloading data')\n",
        "    download(link2data, filename='./files.zip', unzip=True, unzip_path='./')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmuE2u0hOZdw"
      },
      "source": [
        "# <center>521160P Introduction to artificial intelligence<br><br>Exercise material<br><br>Regression<br></center>\n",
        "\n",
        "Machine learning is part of artificial intelligence which can be divided in three sub-sections: supervised learning, unsupervised learning and reinforcement learning. In supervised learning the model is taught with data which has both data samples and output variables available. After teaching, the model is able to predict which output variable the new unknown data sample will receive. In unsupervised learning the model is taught without output variables therefore the model learns structure of data. In reinforcement learning the model has neither data or output variables available instead the model gets either positive or negative feedback upon taken action. Based on feedback the model received, it learns through trial and error trying to find most optimal strategy for task at hand.\n",
        "\n",
        "Supervised Learning is further divided into classification and regression. Regression is a statistical method that estimates the dependence of the data input variables $X$ = $[x_ {1}, x_ {2}, ..., x_ {n}]$ and the output variable $Y$. In regression, the values of the input variables can be either continuous or discrete, but the value of the output variable is always constant. The simplest regression problem deals with one explanatory variable (or dependent variable) $Y$ that depends on only one explanatory variable (or independent variable) $X$. The task is to find a model for this statistical problem that best describes the relationship between $X$ and $Y$ based on the available data. The basic questions of regression analysis are: What mathematical model should be used to solve the problem (straight line, parabola, logarithmic function, etc.) and how do we best fit the model to the graph?\n",
        "\n",
        "## Regression analysis\n",
        "\n",
        "In linear regression, the first-order polynomial function of Equation (1) is used as a model. The purpose is to find the optimal values for the parameters k and b based on the data.\n",
        "\n",
        "\\begin{equation}\n",
        "y = kx + b\\:, \\tag{1}\n",
        "\\end{equation}\n",
        "\n",
        "*where k is the slope and b is the intersection of the y-axis.*\n",
        "\n",
        "In some cases, a higher-degree polynomial function better describes the properties of the data than a linear line. The simplest extension from linear first-degree polynomial function is a second-degree polynomial function or a parabola. Higher order polynomial functions are obtained by adding the terms $x^2$, $x^3$, $...$, $x^n$ to the linear regression model of equation (1) according to equation (2).\n",
        "\n",
        "\\begin{equation}\n",
        "y = a_{0} + a_{1}x + a_{2}x^2 + ... + a_{k}x^k \\tag{2}\n",
        "\\end{equation}\n",
        "\n",
        "If the data allows you to draw a curve through all the data points, there will be no problem in finding the most suitable model. This is not the case in realife. For example, the same model-year cars tend not to be exactly the same price. For this reason, not every single y-coordinate value can be completely predicted from the x-coordinate values, and the prediction includes an error term i.e. a residual.\n",
        "\n",
        "Fitting a mathematical model to data points is a long-debated problem and is usually solved using the least squares method. The least squares method uses L2 normalization, which finds the optimal parameters for the model to be fitted by minimizing the sum of the squares of the vertical error terms. Another alternative method of normalization is L1 normalization, which calculates the absolute value of the vertical error terms. In the two-dimensional case, [L1-norm](https://mathworld.wolfram.com/L1-Norm.html) (manhattan-norm) and [L2-norm](https://mathworld.wolfram.com/L2-Norm.html) (Euclidean norm) is calculated by Equations (3) and (4).\n",
        "\n",
        "\\begin{equation}\n",
        "L1_{norm} = \\sum\\limits_{i=1}^n | Y_{i} - f(X_{i}) | \\tag{3}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "L2_{norm} = \\sqrt{\\sum\\limits_{i=1}^n ( Y_{i} - f(X_{i}) ) ^2} \\:, \\tag{4}\n",
        "\\end{equation}\n",
        "\n",
        "*where $Y_ {i}$ is the y-coordinate value of the data sample and $f(X_ {i})$ is the y-coordinate value predicted by the fitted function.*\n",
        "\n",
        "The smaller the squared or absolute sum of the error terms, the closer the fitted function is to the data samples. In Figure 1, a linear least squares method is fitted to the data by the least squares method. The black dots on the graph are data samples, the red lines are vertical error terms, and the blue line is a straight line.\n",
        "\n",
        "<br>\n",
        "<div style=\"width:image width px; font-size:80%; text-align:center;\">\n",
        "    <center>\n",
        "    <img src='https://github.com/simulate111/Johdatus-teko-lyyn-Introduction-to-Artificial-Intelligence/blob/main/imgs/regressio1.png?raw=1' width='550' height='auto' style='padding-bottom:0.5em;' />\n",
        "    </center>\n",
        "    <span>Figure 1. Line fitted using least squares method minimising sum of squared error terms</span>\n",
        "</div>\n",
        "<br>\n",
        "\n",
        "The parameters of the function to be fitted by the least squares method are calculated utilizing matrix algebra according to equation (5).\n",
        "\n",
        "\\begin{equation}\n",
        "Y = XA \\Leftrightarrow A = (X^TX)^{-1}X^TY\\:, \\tag{5}\n",
        "\\end{equation}\n",
        "\n",
        "*where $X$ is the values of the x-coordinate of the data in the matrix, $Y$ is the values of the y-coordinate of the data in the matrix and $A$ is the coefficient matrix with reference to the parameters [$a_{0}, a_{1}, a_{2}, _, a_{k}$].*\n",
        "\n",
        "The square of the correlation coefficient $r^2$ and the mean squared error (MSE) are used to measure the performance of the fitted model. The statistical measure used tells how well the data samples and the fitted model match. When the square of the correlation coefficient is 1, then there is a complete dependence between the variables of the test data and they can be predicted completely correctly by the model. When the square of the correlation coefficient is 0, the variables do not depend on each other at all, and the fitted model does not correspond to the dependence between them.\n",
        "\n",
        "The mean square error, in turn, gets large values when the data samples deviate much from the matched model. It is 0 when the values of the test data variables can be completely predicted by the model. The square of the correlation coefficient for the fitted model is calculated using Equations (6), (7), (8), (9) and (10). The mean square error for the fitted model is calculated by Equation (11).\n",
        "\n",
        "\\begin{equation}\n",
        "SSE = \\sum\\limits_{i=1}^n (Y_{i} - f(X_{i}))^2 \\tag{6}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "SST = \\sum\\limits_{i=1}^n (Y_{i} - \\bar{Y})^2 \\tag{7}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "SSR = \\sum\\limits_{i=1}^n (f(X_{i}) - \\bar{Y})^2 \\tag{8}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "SST = SSE + SSR \\tag{9}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "r^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST} = 1 - \\frac{\\sum\\limits_{i=1}^n (Y_{i} - f(X_{i}))^2}{\\sum\\limits_{i=1}^n (Y_{i} - \\bar{Y})^2} \\tag{10}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "MSE = \\frac{1}{n} \\sum\\limits_{i=1}^n (Y_{i} - f(X_{i}))^2\\:, \\tag{11}\n",
        "\\end{equation}\n",
        "\n",
        "*where SSE (the sum of squared errors) is the sum of the squared error terms. The total sum of squares (SST) is the sum of the squared differences between the y-coordinate values of the data samples and their mean. Sum of squared differences between the predicted y-coordinate values of the SSR (sum of squares due to regression) and the mean of the y-coordinate values of the data samples*.\n",
        "\n",
        "For example, a quadratic equation of the form $y = a_{0} + a_{1}x + a_{2}x^2$ is used as a model and the model is fitted to the points (2,5), (-2,5), ( 0.0) and (0.2). Then the matrices $X$, $Y$ and $A$ take the form:\n",
        "\n",
        "\\begin{equation*}\n",
        "X = \\begin{bmatrix}\n",
        "1 & 2 & 4\\\\\n",
        "1 & -2 & 4\\\\\n",
        "1 & 0 & 0\\\\\n",
        "1 & 0 & 0\n",
        "\\end{bmatrix},\n",
        "\\:\\:\n",
        "Y = \\begin{bmatrix}\n",
        "5\\\\\n",
        "5\\\\\n",
        "0\\\\\n",
        "2\n",
        "\\end{bmatrix},\n",
        "\\:\\:\n",
        "A = \\begin{bmatrix}\n",
        "a_{0}\\\\\n",
        "a_{1}\\\\\n",
        "a_{2}\n",
        "\\end{bmatrix}\n",
        "\\end{equation*}\n",
        "\n",
        "Using equation (5) we can calculate:\n",
        "\n",
        "\\begin{equation*}\n",
        "A = (X^TX)^{-1}X^TY\n",
        "= \\begin{bmatrix}\n",
        "0.5 & 0 & -0.125\\\\\n",
        "0 & 0.125 & 0\\\\\n",
        "-0.125 & 0 & 0.625\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "1 & 1 & 1 & 1\\\\\n",
        "2 & -2 & 0 & 0\\\\\n",
        "4 & 4 & 0 & 0\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "5\\\\\n",
        "5\\\\\n",
        "0\\\\\n",
        "2\n",
        "\\end{bmatrix}=\n",
        "\\begin{bmatrix}\n",
        "1\\\\\n",
        "0\\\\\n",
        "1\n",
        "\\end{bmatrix}\n",
        "\\end{equation*}\n",
        "\n",
        "The coefficient matrix A can be solved $A =[1 \\: 0 \\: 1]^T$, so the quadratic equation becomes $y = 1 + 0\\cdot x + 1\\cdot x^2$. Calculate the square of the correlation coefficient and the mean square error of the fitted model for the teaching data using Equations (10) and (11):\n",
        "\n",
        "\\begin{equation*}\n",
        "r^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{\\sum\\limits_{i=1}^n (Y_{i} - f(X_{i}))^2}{\\sum\\limits_{i=1}^n (Y_{i} - \\bar{Y})^2} = 1 - \\frac{(5-5)^2 + (5-5)^2 + (0-1)^2 + (2-1)^2}{(5-3)^2 + (5-3)^2 + (0-3)^2 + (2-3)^2} \\approx 0,889\n",
        "\\end{equation*}\n",
        "\n",
        "\\begin{equation*}\n",
        "MSE = \\frac{1}{n} \\sum\\limits_{i=1}^n (Y_{i} - f(X_{i}))^2 = \\frac{(5-5)^2 + (5-5)^2 + (0-1)^2 + (2-1)^2}{4} = 0,5\n",
        "\\end{equation*}\n",
        "\n",
        "Overfitting is a common problem in machine learning and especially in supervised learning. If the model to be fitted is too complex, such as a polynomial function of too high a degree, it starts to model the noise in the data, i.e. the oscillation, and overfitting takes place. This has happened in the graph on the left of Figure 2. Although the model passes through almost all data points and the correlation coefficient of the teaching data is ~ 1 and the mean square error is ~ 0, it is still not the best possible model to model the dependence between the variables.\n",
        "\n",
        "If on the other hand a too simple model is chosen for complex data, it will not be able to mimic the structure of the data accurately enough it's called underfitting. This has happened in the graph to the right of Figure 2. Appropriate matching avoids both of these undesirable phenomena.\n",
        "\n",
        "<br>\n",
        "<div style=\"width:image width px; font-size:80%; text-align:center;\">\n",
        "    <center>\n",
        "    <img src='https://github.com/simulate111/Johdatus-teko-lyyn-Introduction-to-Artificial-Intelligence/blob/main/imgs/regressio2.png?raw=1' width='1100' height='auto' style='padding-bottom:0.5em;' />\n",
        "    </center>\n",
        "    <span>Kuva 2. Overfitted model(left), Appropriately fitted model(middle) ja underfitted model(right) for regression problem.</span>\n",
        "</div>\n",
        "<br>\n",
        "\n",
        "## Logistic regression\n",
        "\n",
        "Logistic regression is a special case of regression analysis that seeks to predict with what probability a defined event will occur. It is a nonlinear regression method commonly used despite its name to classify data. Samples of teaching data must therefore have information about the classes to which they belong. If there are only two categories to be classified a binary logistic regression is used which logistic sigmoid function can be calculated via Equation (12).\n",
        "\n",
        "\\begin{equation}\n",
        "P(Y=1) = \\frac{1}{1+e^{-t}}\\:, \\tag{12}\n",
        "\\end{equation}\n",
        "\n",
        "*where $t = a_{0} + a_{1}x + ⋯ + a_{k}x^k$ and $x$ is an explanatory variable.*\n",
        "\n",
        "In the case of two classes, the counterpart of the event $P(Y = 1)$ can be calculated $P(Y=0)= 1\\:-\\:P(Y=1)$. If there are more than two classes, a multinomial logistic regression is used, the logistic softmax function of which is calculated by Equation (13).\n",
        "\n",
        "\\begin{equation}\n",
        "P(Y=i) = \\frac{e^{\\:t_i}}{\\sum\\limits_{n=1}^N e^{\\:t_n}}\\:, \\tag{13}\n",
        "\\end{equation}\n",
        "\n",
        "*where $t_i = a_{0i}+a_{1i}x+⋯+a_{ki}x^k$, $x$ is the explanatory variable and N is the number of classes\n",
        "\n",
        "As an example, we study how the seed size of a plant affects its probability of germination. This is a case of two categories, as the seed either germinates or does not germinate. Table 1 shows data consisting of 25 seed samples, with 10 seeds germinating and 15 not germinating. [1]\n",
        "\n",
        "\n",
        "<br>*Table 1. Seed data*\n",
        "\n",
        "| <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> | <br> |\n",
        "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
        "| **Seed size (mm)** | 13,8 | 14,8 | 16,3 | 17,0 | 17,3 | 18,2 | 18,3 | 18,9 | 19,3 | 19,6 | 19,9 | 20,3 | 20,4 | 21,1 | 21,2 | 21,3 | 21,4 | 21,6 | 21,8 | 23,2 | 24,3 | 24,9 | 25,1 | 26,0 | 27,6 |\n",
        "| **Seed germinated? (y/n)** | n | n | n | n | n | n | n | y | n | n | y | n | n | n | n | y | y | n | y | n | y | y | y | y | y |\n",
        "<br>\n",
        "\n",
        "Teaching the logistic regression classifier gives the logistic function $P(Y=1)=\\frac{1}{1 + e^{- 0.65x + 14}}$. It is now possible to estimate for example the germination probability of a 21.0 mm seed germinating. Figure 3 plots the logistic function learned on the graph and the samples in Table 1. In the graph, a germination probability value of 1.0 means that the seed germinates with 100% probability and 0.0 that the seed germinates with 0% probability. It can be seen from the graph that based on the logistic regression model, a 21.0 mm seed will germinate with a probability of about 40%.\n",
        "\n",
        "\n",
        "<br>\n",
        "<div style=\"width:image width px; font-size:80%; text-align:center;\">\n",
        "    <center>\n",
        "    <img src='https://github.com/simulate111/Johdatus-teko-lyyn-Introduction-to-Artificial-Intelligence/blob/main/imgs/regressio3.png?raw=1' width='600' height='auto' style='padding-bottom:0.5em;' />\n",
        "    </center>\n",
        "    <span>Figure 3. Seed size (x-axis) - germination propability (y-axis) graph.</span>\n",
        "</div>\n",
        "<br>\n",
        "\n",
        "## Sources\n",
        "\n",
        "[1] Vanhoenacker D. Logistic Regression. URL:http://hem.bredband.net/didrik71/recostat/logreg_e.htm.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}